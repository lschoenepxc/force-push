{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAIL Hackathon 2024\n",
    "\n",
    "Team: Force Push\n",
    "Members: Martin Doppstadt, Simon Böke, Philip Holstein, Felix Schnüll, Laura Schöne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start: Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import initial_data.csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# import data from csv file\n",
    "def import_data():\n",
    "    data = pd.read_csv('initial_data.csv')\n",
    "    return data\n",
    "\n",
    "data = import_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import queried data\n",
    "\n",
    "def read_csv_file(file_path):\n",
    "    # Initialisiere eine leere Liste, um die bereinigten Daten zu speichern\n",
    "    cleaned_data = []\n",
    "\n",
    "    # Lese die CSV-Datei\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')  # Annahme: Semikolon als Trennzeichen\n",
    "        for row in reader:\n",
    "            # Verbinde die Zeilenelemente mit einem Komma, um das Trennzeichen zu vereinheitlichen\n",
    "            unified_row = ','.join(row)\n",
    "            # Ersetze mehrere aufeinander folgende Kommas durch ein einzelnes Komma\n",
    "            unified_row = re.sub(r',+', ',', unified_row)\n",
    "            # Teile die vereinheitlichte Zeile nach dem Komma auf\n",
    "            split_row = unified_row.split(',')\n",
    "            # Entferne die ersten zwei Parameter\n",
    "            cleaned_row = split_row[1:]\n",
    "            # Füge die bereinigte Zeile der Liste hinzu\n",
    "            cleaned_data.append(cleaned_row)\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "# Beispiel: Daten aus \"input.csv\" einlesen\n",
    "input_file_path = \"querys_ForcePush.csv\"\n",
    "cleaned_data = read_csv_file(input_file_path)\n",
    "\n",
    "# Bereinigte Daten als pandas DataFrame speichern mit erster Zeile als Spaltennamen\n",
    "df_queried = pd.DataFrame(cleaned_data[1:], columns=cleaned_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add initial_data and querys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data(data, queried_data):\n",
    "    # add queried data (without cost) to initial data\n",
    "    data = pd.concat([data, queried_data.iloc[:, :13]], axis=0)\n",
    "    return data\n",
    "\n",
    "data = add_data(data, df_queried)\n",
    "# print(data)\n",
    "\n",
    "data = data.astype(float)\n",
    "# print(data.columns)\n",
    "# drop PM2 column\n",
    "data = data.drop(columns=['PM 2'])\n",
    "# print(data.columns)\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to handle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_data(data, row):\n",
    "    x1, x2, x3, x4, x5, x6, x7, x8 = data.iloc[row, 0], data.iloc[row, 1], data.iloc[row, 2], data.iloc[row, 3], data.iloc[row, 4], data.iloc[row, 5], data.iloc[row, 6], data.iloc[row, 7]\n",
    "    return x1, x2, x3, x4, x5, x6, x7, x8\n",
    "\n",
    "def get_critical_output_data(data, i):\n",
    "    x1, x2 = data.iloc[i, 9], data.iloc[i, 11]\n",
    "    return x1, x2\n",
    "\n",
    "# print data column 9 and 11 (PM 1 and Pressure cylinder)\n",
    "# print(data.iloc[:, 9])\n",
    "# print(data.iloc[:, 11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs and Outputs columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:  ['Engine speed' 'Engine load' 'Railpressure' 'Air supply' 'Crank angle'\n",
      " 'Intake pressure' 'Back pressure' 'Intake temperature']\n",
      "Outputs:  ['NOx' 'PM 1' 'CO2' 'Pressure cylinder']\n"
     ]
    }
   ],
   "source": [
    "# put column names into a list\n",
    "column_names = data.columns.values\n",
    "#print(column_names)\n",
    "inputs = column_names[0:8]\n",
    "print(\"Inputs: \", inputs)\n",
    "outputs = column_names[8:12]\n",
    "print(\"Outputs: \", outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify safe data and add label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify data safety\n",
    "# Check if outputs are in safe range\n",
    "# PM 1 < 6, (PM 2 < 16), Pressure cylinder < 160\n",
    "\n",
    "def label_safe(data):\n",
    "    safe = []\n",
    "    for i in range(len(data)):\n",
    "        x9, x10  = get_critical_output_data(data, i)\n",
    "        if x9 < 6:\n",
    "            if x10 < 160:\n",
    "                safe.append(0)\n",
    "            else:\n",
    "                safe.append(2)\n",
    "        else:\n",
    "            if x10 < 160:\n",
    "                safe.append(1)\n",
    "            else:\n",
    "                safe.append(2)\n",
    "    data['safe'] = safe\n",
    "    return data\n",
    "\n",
    "data = label_safe(data)\n",
    "#print(data['safe'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train RandomForestClassifier to predict safe and unsafe data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unsafe in training:  39\n",
      "Number of unsafe in testing:  9\n",
      "Accuracy: 0.9113924050632911\n"
     ]
    }
   ],
   "source": [
    "# train a model to predict unsafe output\n",
    "X = data[inputs]\n",
    "y = data['safe']\n",
    "\n",
    "# split data into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Number of unsafe in training: \",(len(y_train[y_train == 1])+len(y_train[y_train == 2])))\n",
    "print(\"Number of unsafe in testing: \",(len(y_test[y_test == 1])+len(y_test[y_test == 2])))\n",
    "\n",
    "# train a random forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Submission-Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import submission data\n",
    "submission_data = pd.read_csv('submission.csv')\n",
    "# print(submission_data)\n",
    "# rename columns to same columns as complete data\n",
    "submission_data.columns = column_names[:8]\n",
    "# print(submission_data.columns)\n",
    "# print(submission_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check possible new data points from submissions for safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54075\n",
      "57892\n"
     ]
    }
   ],
   "source": [
    "new_data = pd.DataFrame(submission_data, columns=inputs)\n",
    "\n",
    "# from feasibility import is_feasible\n",
    "# Feasibility was checked beforehand for data in submission.csv --> all data points are feasible\n",
    "\n",
    "# check newData safety via classifier\n",
    "X = new_data\n",
    "# print(X.head())\n",
    "safeDataPoints = []\n",
    "y_pred = clf.predict(X)\n",
    "# print(y_pred)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == 0:\n",
    "        # append the corresponding data point to safeDataPoints\n",
    "        safeDataPoints.append(X.iloc[i])\n",
    "        \n",
    "print(len(y_pred[y_pred == 0]))\n",
    "print(len(y_pred))\n",
    "# print(len(safeDataPoints) == len(y_pred[y_pred == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find significant data based on complete \"real\" data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0]\n",
      "1005.6904345750808;22.860694856250767;1537.7016626647037;463.57024063494424;0.4202272556722164;1805.398915270618;1508.9104433589675;59.62588668260171\n",
      "1838.9744520187376;14.896431590448667;2241.7939017306735;664.9706615381898;9.731257443798269;2388.3050230270883;3199.2351766715774;67.83688051182946\n",
      "1686.7507934570312;133.74633660763865;2363.175705237151;984.6811989482665;-7.850949778205514;1708.7789804889453;2960.2142682774747;52.002105463977834\n",
      "2242.3091664910316;95.669230241097;2125.724965978517;1208.109805903974;-1.861551116707851;2994.345297157826;2919.615742479734;44.292533842106145\n",
      "1248.1164753437042;67.74137770757079;1197.761941091458;696.0805674723846;-7.835074495524168;1476.4820741136205;1359.7904681209757;60.469301900945354\n",
      "1254.9142211675644;4.751905631273985;2011.0473093833705;525.8536759920767;-2.8714774013496935;1932.0728100116123;3433.1271588530103;74.19617957680015\n"
     ]
    }
   ],
   "source": [
    "# Schritt 1: Feature-Importance bestimmen\n",
    "complData = data\n",
    "X = complData[inputs]\n",
    "y = complData[outputs]\n",
    "#print(y.head())\n",
    "model = RandomForestRegressor(n_estimators=100)\n",
    "model.fit(X, y)\n",
    "feature_importances = model.feature_importances_\n",
    "# print(feature_importances)\n",
    "\n",
    "# Schritt 2: Datenpunkte basierend auf Feature-Wichtigkeiten anpassen\n",
    "adjustedData = X * feature_importances\n",
    "\n",
    "# Schritt 3: Clusteranalyse durchführen mit angepassten Daten\n",
    "scaler = StandardScaler()\n",
    "scaledAdjustedData = scaler.fit_transform(adjustedData)\n",
    "\n",
    "\n",
    "# Schritt 3.1: Elbow method to determine number of clusters\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "# inertia = []\n",
    "# for i in range(1, 10):\n",
    "#     kmeans = KMeans(n_clusters=i)\n",
    "#     kmeans.fit(scaledAdjustedData)\n",
    "#     print(i, kmeans.inertia_)\n",
    "#     inertia.append(kmeans.inertia_)\n",
    "# plt.plot(range(1, 10), inertia)\n",
    "# plt.show()\n",
    "\n",
    "kmeans = KMeans(n_clusters=6)\n",
    "clusterLabels = kmeans.fit_predict(scaledAdjustedData)\n",
    "\n",
    "# Schritt 4: Repräsentative Datenpunkte auswählen in der Nähe der Clusterzentren\n",
    "\n",
    "# Schritt 4.1: Datenpunkte aus submission.csv auswählen, die noch nicht gequeriet wurden\n",
    "\n",
    "submission_data_filtered = submission_data.copy()\n",
    "is_duplicate = submission_data_filtered[inputs].apply(tuple, 1).isin(data[inputs].apply(tuple, 1))\n",
    "submission_data_filtered = submission_data_filtered[~is_duplicate]\n",
    "\n",
    "#print len of is duplicates True\n",
    "#print(len(is_duplicate[is_duplicate == True]))\n",
    "\n",
    "submission_data_filtered['cluster'] = kmeans.predict(scaler.transform(submission_data_filtered[inputs] * feature_importances))\n",
    "new_points = []\n",
    "for cluster in range(6):\n",
    "    cluster_data = submission_data_filtered[submission_data_filtered['cluster'] == cluster]\n",
    "    cluster_center = kmeans.cluster_centers_[cluster]\n",
    "    # Berechne die Distanz unter Berücksichtigung der Feature-Wichtigkeiten\n",
    "    closest_points, _ = pairwise_distances_argmin_min(cluster_data[inputs] * feature_importances, [cluster_center])\n",
    "    # get the closest point\n",
    "    if len(closest_points) > 0:\n",
    "        new_points.append(cluster_data.iloc[closest_points[0]])\n",
    "new_points = pd.DataFrame(new_points)\n",
    "# print(new_points)\n",
    "\n",
    "new_data = pd.DataFrame(new_points, columns=inputs)\n",
    "\n",
    "# check newDataPoints safety via classifier\n",
    "X = new_data\n",
    "# print(X.head())\n",
    "safeDataPoints = []\n",
    "y_pred = clf.predict(X)\n",
    "print(y_pred)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == 0:\n",
    "        # append the corrsponding data point to safeDataPoints\n",
    "        safeDataPoints.append(X.iloc[i])\n",
    "        \n",
    "\n",
    "# print(len(safeDataPoints) == len(y_pred[y_pred == 0]))\n",
    "\n",
    "# print as formatted string seperated by semi-colon (to ease copy-pasting for Queries)\n",
    "def print_data(data):\n",
    "    for i in range(len(data)):\n",
    "        print(';'.join(map(str, data[i])))\n",
    "    return\n",
    "\n",
    "print_data(safeDataPoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR Model with hyperparameter Search, Training, Prediction (and Cross-Validation) for each output Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauraschone/Desktop/repos/force-push/SVRModel.py:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data = pd.concat([data, queried_data.iloc[:, :13]], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter:  0 {'estimator__C': 200, 'estimator__epsilon': 1, 'estimator__kernel': 'rbf'}\n",
      "Bester Score: 0 -2249.8713465897\n",
      "Accuracy i:  84.67072391084065\n",
      "Predictions i:  [[114.24412075]\n",
      " [171.11336468]\n",
      " [204.65273225]\n",
      " [229.18417868]\n",
      " [140.63972541]\n",
      " [176.02921462]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauraschone/Desktop/repos/force-push/SVRModel.py:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data = pd.concat([data, queried_data.iloc[:, :13]], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter:  1 {'estimator__C': 10, 'estimator__epsilon': 0.1, 'estimator__kernel': 'rbf'}\n",
      "Bester Score: 1 -0.36127943365884607\n",
      "Accuracy i:  46.44478470811493\n",
      "Predictions i:  [[1.7476009 ]\n",
      " [1.66733296]\n",
      " [1.86765163]\n",
      " [2.42161355]\n",
      " [2.08942984]\n",
      " [1.37685393]]\n",
      "Beste Parameter:  2 {'estimator__C': 200, 'estimator__epsilon': 1, 'estimator__kernel': 'linear'}\n",
      "Bester Score: 2 -15.386471572008166\n",
      "Accuracy i:  85.00918056263451\n",
      "Predictions i:  [[41.42653994]\n",
      " [57.42478728]\n",
      " [76.28162022]\n",
      " [80.42245231]\n",
      " [61.67628244]\n",
      " [60.49642592]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauraschone/Desktop/repos/force-push/SVRModel.py:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data = pd.concat([data, queried_data.iloc[:, :13]], axis=0)\n",
      "/Users/lauraschone/Desktop/repos/force-push/SVRModel.py:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data = pd.concat([data, queried_data.iloc[:, :13]], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter:  3 {'estimator__C': 200, 'estimator__epsilon': 0.01, 'estimator__kernel': 'linear'}\n",
      "Bester Score: 3 -7.7757278450491\n",
      "Accuracy i:  97.39531667162007\n",
      "Predictions i:  [[ 62.70414746]\n",
      " [148.70949498]\n",
      " [156.11861981]\n",
      " [130.98018116]\n",
      " [ 50.17602396]\n",
      " [145.83348296]]\n",
      "Beste Parameter:  0 {'estimator__C': 200, 'estimator__epsilon': 1, 'estimator__kernel': 'rbf'}\n",
      "Bester Score: 0 -2249.8713465897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauraschone/Desktop/repos/force-push/SVRModel.py:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data = pd.concat([data, queried_data.iloc[:, :13]], axis=0)\n",
      "/Users/lauraschone/Desktop/repos/force-push/.venv/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/lauraschone/Desktop/repos/force-push/SVRModel.py:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data = pd.concat([data, queried_data.iloc[:, :13]], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter:  1 {'estimator__C': 10, 'estimator__epsilon': 0.1, 'estimator__kernel': 'rbf'}\n",
      "Bester Score: 1 -0.36127943365884607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauraschone/Desktop/repos/force-push/.venv/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/lauraschone/Desktop/repos/force-push/SVRModel.py:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data = pd.concat([data, queried_data.iloc[:, :13]], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter:  2 {'estimator__C': 200, 'estimator__epsilon': 1, 'estimator__kernel': 'linear'}\n",
      "Bester Score: 2 -15.386471572008166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauraschone/Desktop/repos/force-push/.venv/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/lauraschone/Desktop/repos/force-push/SVRModel.py:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data = pd.concat([data, queried_data.iloc[:, :13]], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter:  3 {'estimator__C': 200, 'estimator__epsilon': 0.01, 'estimator__kernel': 'linear'}\n",
      "Bester Score: 3 -7.7757278450491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauraschone/Desktop/repos/force-push/.venv/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from SVRModel import getSVRPrediction\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "for i in range(0,4):\n",
    "    # feedback = getSVRPrediction(safeDataPoints, i, hyperParas[i])\n",
    "    feedback = getSVRPrediction(safeDataPoints, i)\n",
    "    print(\"Accuracy i: \", feedback[0])\n",
    "    print(\"Predictions i: \", feedback[1])\n",
    "\n",
    "# make predicition for all data points in submission.csv\n",
    "# Outputs:  ['NOx' 'PM 1' 'CO2' 'Pressure cylinder']\n",
    "nox = []\n",
    "pm1 = []\n",
    "co2 = []\n",
    "pressure = []\n",
    "predictions = [nox, pm1, co2, pressure]\n",
    "\n",
    "submission_data = pd.read_csv('submission.csv')\n",
    "# rename columns to same columns as complete data\n",
    "submission_data.columns = column_names[:8]\n",
    "submission_data = pd.DataFrame(submission_data, columns=inputs)\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    feedback = getSVRPrediction(submission_data, i)\n",
    "    predictions[i] = feedback[1]\n",
    "for i in range(len(predictions)):\n",
    "    predictions[i] = pd.DataFrame(predictions[i], columns=[outputs[i]])\n",
    "\n",
    "submission_data['NOx'] = predictions[0]\n",
    "submission_data['PM 1'] = predictions[1]\n",
    "submission_data['CO2'] = predictions[2]\n",
    "submission_data['Pressure cylinder'] = predictions[3]\n",
    "\n",
    "# write to csv\n",
    "submission_data.to_csv('submission_final.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other models that did not make it to final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model from Simon\n",
    "# from PLSFromSimon import getPLSPrediction\n",
    "\n",
    "# # print(getPLSPrediction(safeDataPoints))\n",
    "# print(\"Accuracy: \", getPLSPrediction(safeDataPoints)[0])\n",
    "# print(\"Predictions: \", getPLSPrediction(safeDataPoints)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tryRidge import getRidgePrediction\n",
    "\n",
    "# print(\"Accuracy: \", getRidgePrediction(safeDataPoints)[0])\n",
    "# print(\"Predictions: \", getRidgePrediction(safeDataPoints)[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
