{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start: Analysiere Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import initial_data.csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from feasibility import is_feasible\n",
    "\n",
    "# import data from csv file\n",
    "def import_data():\n",
    "    data = pd.read_csv('initial_data.csv')\n",
    "    return data\n",
    "\n",
    "data = import_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import queried data\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def read_csv_file(file_path):\n",
    "    # Initialisiere eine leere Liste, um die bereinigten Daten zu speichern\n",
    "    cleaned_data = []\n",
    "\n",
    "    # Lese die CSV-Datei\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')  # Annahme: Semikolon als Trennzeichen\n",
    "        for row in reader:\n",
    "            # Verbinde die Zeilenelemente mit einem Komma, um das Trennzeichen zu vereinheitlichen\n",
    "            unified_row = ','.join(row)\n",
    "            # Ersetze mehrere aufeinander folgende Kommas durch ein einzelnes Komma\n",
    "            unified_row = re.sub(r',+', ',', unified_row)\n",
    "            # Teile die vereinheitlichte Zeile nach dem Komma auf\n",
    "            split_row = unified_row.split(',')\n",
    "            # Entferne die ersten zwei Parameter\n",
    "            cleaned_row = split_row[1:]\n",
    "            # Füge die bereinigte Zeile der Liste hinzu\n",
    "            cleaned_data.append(cleaned_row)\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "# Beispiel: Daten aus \"input.csv\" einlesen\n",
    "input_file_path = \"querys_ForcePush.csv\"\n",
    "cleaned_data = read_csv_file(input_file_path)\n",
    "\n",
    "# Bereinigte Daten als pandas DataFrame speichern mit erster Zeile als Spaltennamen\n",
    "df_queried = pd.DataFrame(cleaned_data[1:], columns=cleaned_data[0])\n",
    "\n",
    "\n",
    "# Bereinigte Daten ausgeben\n",
    "# print(df_queried)\n",
    "# df_queried = pd.DataFrame(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Engine speed', 'Engine load', 'Railpressure', 'Air supply',\n",
      "       'Crank angle', 'Intake pressure', 'Back pressure', 'Intake temperature',\n",
      "       'NOx', 'PM 1', 'CO2', 'PM 2', 'Pressure cylinder'],\n",
      "      dtype='object')\n",
      "Index(['Engine speed', 'Engine load', 'Railpressure', 'Air supply',\n",
      "       'Crank angle', 'Intake pressure', 'Back pressure', 'Intake temperature',\n",
      "       'NOx', 'PM 1', 'CO2', 'Pressure cylinder'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def add_data(data, queried_data):\n",
    "    # add queried data (without cost) to initial data\n",
    "    data = pd.concat([data, queried_data.iloc[:, :13]], axis=0)\n",
    "    return data\n",
    "\n",
    "data = add_data(data, df_queried)\n",
    "# print(data)\n",
    "\n",
    "data = data.astype(float)\n",
    "print(data.columns)\n",
    "# drop PM2 column\n",
    "data = data.drop(columns=['PM 2'])\n",
    "print(data.columns)\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to handle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_data(data, row):\n",
    "    x1, x2, x3, x4, x5, x6, x7, x8 = data.iloc[row, 0], data.iloc[row, 1], data.iloc[row, 2], data.iloc[row, 3], data.iloc[row, 4], data.iloc[row, 5], data.iloc[row, 6], data.iloc[row, 7]\n",
    "    return x1, x2, x3, x4, x5, x6, x7, x8\n",
    "\n",
    "def get_critical_output_data(data, i):\n",
    "    x1, x2 = data.iloc[i, 9], data.iloc[i, 11]\n",
    "    return x1, x2\n",
    "\n",
    "# print data column 9 and 11\n",
    "# print(data.iloc[:, 9])\n",
    "# print(data.iloc[:, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:  ['Engine speed' 'Engine load' 'Railpressure' 'Air supply' 'Crank angle'\n",
      " 'Intake pressure' 'Back pressure' 'Intake temperature']\n",
      "Outputs:  ['NOx' 'PM 1' 'CO2' 'Pressure cylinder']\n"
     ]
    }
   ],
   "source": [
    "# put column names into a list\n",
    "column_names = data.columns.values\n",
    "#print(column_names)\n",
    "inputs = column_names[0:8]\n",
    "print(\"Inputs: \", inputs)\n",
    "outputs = column_names[8:12]\n",
    "print(\"Outputs: \", outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify safe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "158    0\n",
      "159    2\n",
      "160    0\n",
      "161    0\n",
      "162    0\n",
      "Name: safe, Length: 163, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# classify data safety\n",
    "# Check if outputs are in safe range\n",
    "# PM 1 < 6, (PM 2 < 16), Pressure cylinder < 160\n",
    "\n",
    "def label_safe(data):\n",
    "    safe = []\n",
    "    for i in range(len(data)):\n",
    "        x9, x10  = get_critical_output_data(data, i)\n",
    "        if x9 < 6:\n",
    "            if x10 < 160:\n",
    "                safe.append(0)\n",
    "            else:\n",
    "                safe.append(2)\n",
    "        else:\n",
    "            if x10 < 160:\n",
    "                safe.append(1)\n",
    "            else:\n",
    "                safe.append(2)\n",
    "    data['safe'] = safe\n",
    "    return data\n",
    "\n",
    "data = label_safe(data)\n",
    "print(data['safe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unsafe in training:  11\n",
      "Number of unsafe in testing:  3\n",
      "Accuracy: 0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "# train a model to predict unsafe output\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# split data into training and testing data\n",
    "\n",
    "# Schritt 1: Feature-Importance bestimmen\n",
    "X = data[inputs]\n",
    "y = data['safe']\n",
    "#print(y.head())\n",
    "# model = RandomForestRegressor(n_estimators=100)\n",
    "# model.fit(X, y)\n",
    "# feature_importances = model.feature_importances_\n",
    "# print(feature_importances)\n",
    "\n",
    "# # Schritt 2: Datenpunkte basierend auf Feature-Wichtigkeiten anpassen\n",
    "# adjustedData = X * feature_importances\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(adjustedData, y, test_size=0.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#X_train = X_train * feature_importances\n",
    "# count number of False values in y_train and y_test\n",
    "print(\"Number of unsafe in training: \",(len(y_train[y_train == 1])+len(y_train[y_train == 2])))\n",
    "print(\"Number of unsafe in testing: \",(len(y_test[y_test == 1])+len(y_test[y_test == 2])))\n",
    "\n",
    "# train a random forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Submission-Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Engine speed', 'Engine load', 'Railpressure', 'Air supply',\n",
      "       'Crank angle', 'Intake pressure', 'Back pressure',\n",
      "       'Intake temperature'],\n",
      "      dtype='object')\n",
      "   Engine speed  Engine load  Railpressure  Air supply  Crank angle  \\\n",
      "0   2079.242896    15.249091   1059.615682  330.175905    -6.221489   \n",
      "1    820.449848   120.353061   1271.158901  487.645261     3.472795   \n",
      "2   1639.495961    83.300535   1601.817725  634.526366    -0.793814   \n",
      "3   1260.782708   101.754809    828.257891  392.354655     0.122483   \n",
      "4    903.744741    30.628885   1650.039653  602.760187     6.470709   \n",
      "\n",
      "   Intake pressure  Back pressure  Intake temperature  \n",
      "0      1126.065139    3432.348069           73.435318  \n",
      "1      2084.175034    1985.181081           42.863344  \n",
      "2      1268.308573    3234.188679           50.848907  \n",
      "3      2492.140669    1466.726258           72.014909  \n",
      "4      2631.683432    3797.403335           56.749198  \n"
     ]
    }
   ],
   "source": [
    "# import submission data\n",
    "submission_data = pd.read_csv('submission.csv')\n",
    "# print(submission_data)\n",
    "# rename columns\n",
    "submission_data.columns = column_names[:8]\n",
    "print(submission_data.columns)\n",
    "print(submission_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Engine speed  Engine load  Railpressure  Air supply  Crank angle  \\\n",
      "0   2079.242896    15.249091   1059.615682  330.175905    -6.221489   \n",
      "1    820.449848   120.353061   1271.158901  487.645261     3.472795   \n",
      "2   1639.495961    83.300535   1601.817725  634.526366    -0.793814   \n",
      "3   1260.782708   101.754809    828.257891  392.354655     0.122483   \n",
      "4    903.744741    30.628885   1650.039653  602.760187     6.470709   \n",
      "\n",
      "   Intake pressure  Back pressure  Intake temperature  \n",
      "0      1126.065139    3432.348069           73.435318  \n",
      "1      2084.175034    1985.181081           42.863344  \n",
      "2      1268.308573    3234.188679           50.848907  \n",
      "3      2492.140669    1466.726258           72.014909  \n",
      "4      2631.683432    3797.403335           56.749198  \n",
      "[0 0 0 ... 0 0 0]\n",
      "57352\n",
      "57892\n"
     ]
    }
   ],
   "source": [
    "new_data = pd.DataFrame(submission_data, columns=inputs)\n",
    "\n",
    "# check feasibleData safety via classifier\n",
    "X = new_data\n",
    "print(X.head())\n",
    "safeDataPoints = []\n",
    "y_pred = clf.predict(X)\n",
    "print(y_pred)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == 0:\n",
    "        #append the corrsponding data point to safeDataPoints\n",
    "        safeDataPoints.append(X.iloc[i])\n",
    "        \n",
    "print(len(y_pred[y_pred == 0]))\n",
    "print(len(y_pred))\n",
    "# print(len(safeDataPoints) == len(y_pred[y_pred == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find significant data based on complete real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01654377 0.56176859 0.05609762 0.09215921 0.11147774 0.06603134\n",
      " 0.03571821 0.06020353]\n",
      "13099\n",
      "4017\n",
      "8034\n",
      "3652\n",
      "15445\n",
      "13595\n",
      "    Engine speed  Engine load  Railpressure   Air supply  Crank angle  \\\n",
      "8    2212.890089    54.532932   1473.622894  1266.422615    -3.144072   \n",
      "20   1237.667766    20.297782   2036.500320   565.600433    -0.005135   \n",
      "23   1414.956297    52.064432    637.906118   911.010384    -3.168024   \n",
      "11    988.948241    38.966357   1471.254208   179.178156     0.869117   \n",
      "4     903.744741    30.628885   1650.039653   602.760187     6.470709   \n",
      "2    1639.495961    83.300535   1601.817725   634.526366    -0.793814   \n",
      "\n",
      "    Intake pressure  Back pressure  Intake temperature  cluster  \n",
      "8       2807.164168    2108.391069           57.550345      0.0  \n",
      "20      1331.763297    1937.479623           70.902561      1.0  \n",
      "23      1726.091088    1635.553227           46.924010      2.0  \n",
      "11      1186.477575    2334.776826           51.094320      3.0  \n",
      "4       2631.683432    3797.403335           56.749198      4.0  \n",
      "2       1268.308573    3234.188679           50.848907      5.0  \n",
      "    Engine speed  Engine load  Railpressure   Air supply  Crank angle  \\\n",
      "8    2212.890089    54.532932   1473.622894  1266.422615    -3.144072   \n",
      "20   1237.667766    20.297782   2036.500320   565.600433    -0.005135   \n",
      "23   1414.956297    52.064432    637.906118   911.010384    -3.168024   \n",
      "11    988.948241    38.966357   1471.254208   179.178156     0.869117   \n",
      "4     903.744741    30.628885   1650.039653   602.760187     6.470709   \n",
      "\n",
      "    Intake pressure  Back pressure  Intake temperature  \n",
      "8       2807.164168    2108.391069           57.550345  \n",
      "20      1331.763297    1937.479623           70.902561  \n",
      "23      1726.091088    1635.553227           46.924010  \n",
      "11      1186.477575    2334.776826           51.094320  \n",
      "4       2631.683432    3797.403335           56.749198  \n",
      "[0 0 0 0 0 0]\n",
      "True\n",
      "2212.890088558197;54.53293222984283;1473.6228935542258;1266.4226146048354;-3.1440719972748754;2807.164168031438;2108.3910688055885;57.55034501802291\n",
      "1237.667766213417;20.29778233729303;2036.5003196091689;565.6004329788941;-0.0051345769315958;1331.763297068353;1937.479623173178;70.90256134332367\n",
      "1414.956296980381;52.064431784674525;637.9061175606587;911.0103843653476;-3.1680236602160914;1726.0910879335695;1635.5532267131955;46.92401016399516\n",
      "988.9482408761978;38.96635679889533;1471.2542082281043;179.17815555396663;0.8691165200434625;1186.477574725457;2334.7768262827503;51.09432000244607\n",
      "903.7447407841682;30.628884836476622;1650.0396527922403;602.7601869229843;6.470708707347512;2631.6834322521067;3797.403334714325;56.7491981415679\n",
      "1639.495961368084;83.30053522160668;1601.8177249196335;634.5263661353208;-0.7938141296199053;1268.3085728382453;3234.188678692356;50.84890673909587\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Schritt 1: Feature-Importance bestimmen\n",
    "complData = data\n",
    "X = complData[inputs]\n",
    "y = complData[outputs]\n",
    "#print(y.head())\n",
    "model = RandomForestRegressor(n_estimators=100)\n",
    "model.fit(X, y)\n",
    "feature_importances = model.feature_importances_\n",
    "print(feature_importances)\n",
    "\n",
    "# Schritt 2: Datenpunkte basierend auf Feature-Wichtigkeiten anpassen\n",
    "adjustedData = X * feature_importances\n",
    "\n",
    "# Schritt 3: Clusteranalyse durchführen mit angepassten Daten\n",
    "scaler = StandardScaler()\n",
    "scaledAdjustedData = scaler.fit_transform(adjustedData)\n",
    "\n",
    "\n",
    "# elbow method to determine number of clusters\n",
    "# from matplotlib import pyplot as plt\n",
    "# inertia = []\n",
    "# for i in range(1, 10):\n",
    "#     kmeans = KMeans(n_clusters=i)\n",
    "#     kmeans.fit(scaledAdjustedData)\n",
    "#     print(i, kmeans.inertia_)\n",
    "#     inertia.append(kmeans.inertia_)\n",
    "# plt.plot(range(1, 10), inertia)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=6)\n",
    "clusterLabels = kmeans.fit_predict(scaledAdjustedData)\n",
    "\n",
    "# Schritt 4: Repräsentative Datenpunkte auswählen\n",
    "# (Beispiel zeigt, wie man die Clusterlabels zu den ursprünglichen Daten hinzufügt und repräsentative Punkte auswählt)\n",
    "# freeData = submission_data.droprows(), wenn die Datenpunkte in data enthalten sind\n",
    "\n",
    "# Angenommen, `inputs` ist eine Liste der Spaltennamen, die für den Vergleich verwendet werden sollen\n",
    "# Erstelle eine temporäre Kopie von `submission_data`, um die Originaldaten nicht zu verändern\n",
    "submission_data_filtered = submission_data.copy()\n",
    "\n",
    "# Schritt 1: Finde Duplikate basierend auf den `inputs` Spalten\n",
    "# Dieser Schritt erzeugt eine Maske (einen Boolean-Array), der für jede Zeile in `submission_data` True ist, wenn diese Zeile in `data` vorhanden ist\n",
    "is_duplicate = submission_data_filtered[inputs].apply(tuple, 1).isin(data[inputs].apply(tuple, 1))\n",
    "\n",
    "# Schritt 2: Lösche die Reihen aus `submission_data_filtered`, die in `data` vorhanden sind\n",
    "submission_data_filtered = submission_data_filtered[~is_duplicate]\n",
    "\n",
    "#print len of is duplicates True\n",
    "#print(len(is_duplicate[is_duplicate == True]))\n",
    "\n",
    "#print(len(submission_data_filtered), len(submission_data), len(data),len(is_duplicate[is_duplicate == True]), len(submission_data)- len(submission_data_filtered))\n",
    "\n",
    "# `submission_data_filtered` enthält jetzt nur die Reihen, die nicht in `data[inputs]` vorhanden sind\n",
    "\n",
    "submission_data_filtered['cluster'] = kmeans.predict(scaler.transform(submission_data_filtered[inputs] * feature_importances))\n",
    "new_points = []\n",
    "for cluster in range(6):\n",
    "    cluster_data = submission_data_filtered[submission_data_filtered['cluster'] == cluster]\n",
    "    cluster_center = kmeans.cluster_centers_[cluster]\n",
    "    # Berechne die Distanz unter Berücksichtigung der Feature-Wichtigkeiten\n",
    "    closest_points, _ = pairwise_distances_argmin_min(cluster_data[inputs] * feature_importances, [cluster_center])\n",
    "    print(len(closest_points))\n",
    "    # get the first 3 closest points\n",
    "    if len(closest_points) > 0:\n",
    "        new_points.append(cluster_data.iloc[closest_points[0]])\n",
    "new_points = pd.DataFrame(new_points)\n",
    "print(new_points)\n",
    "\n",
    "new_data = pd.DataFrame(new_points, columns=inputs)\n",
    "\n",
    "# check feasibleData safety via classifier\n",
    "X = new_data\n",
    "print(X.head())\n",
    "safeDataPoints = []\n",
    "y_pred = clf.predict(X)\n",
    "print(y_pred)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == 0:\n",
    "        #append the corrsponding data point to safeDataPoints\n",
    "        safeDataPoints.append(X.iloc[i])\n",
    "        \n",
    "\n",
    "print(len(safeDataPoints) == len(y_pred[y_pred == 0]))\n",
    "\n",
    "# print as formatted string seperated by semi-colon\n",
    "def print_data(data):\n",
    "    for i in range(len(data)):\n",
    "        print(';'.join(map(str, data[i])))\n",
    "    return\n",
    "\n",
    "print_data(safeDataPoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Engine speed', 'Engine load', 'Railpressure', 'Air supply',\n",
      "       'Crank angle', 'Intake pressure', 'Back pressure', 'Intake temperature',\n",
      "       'NOx', 'PM 1', 'CO2', 'PM 2', 'Pressure cylinder', 'costs'],\n",
      "      dtype='object')\n",
      "[[209.92186555   1.99468288  76.62855937  95.19696292]\n",
      " [100.71029624   1.37645774  48.23837581  68.15850148]\n",
      " [241.59907857   3.0190528   56.70540579  79.16215799]\n",
      " [110.71887287  -0.85990514  36.93346381  76.61078081]\n",
      " [186.48644127  -0.73597376  61.57068879  98.30414441]\n",
      " [249.46998126   3.38396318  78.74895534  89.91076547]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauraschone/Desktop/repos/force-push/SVRFürLaura.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data = pd.concat([data, queried_data.iloc[:, :13]], axis=0)\n"
     ]
    }
   ],
   "source": [
    "# SVR from Philip\n",
    "from SVRFürLaura import getSVRPrediction\n",
    "\n",
    "print(getSVRPrediction(safeDataPoints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model from Simon"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
