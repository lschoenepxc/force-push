{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start: Analysiere Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import initial_data.csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from feasibility import is_feasible\n",
    "\n",
    "# import data from csv file\n",
    "def import_data():\n",
    "    data = pd.read_csv('initial_data.csv')\n",
    "    return data\n",
    "\n",
    "data = import_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import queried data\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def read_csv_file(file_path):\n",
    "    # Initialisiere eine leere Liste, um die bereinigten Daten zu speichern\n",
    "    cleaned_data = []\n",
    "\n",
    "    # Lese die CSV-Datei\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')  # Annahme: Semikolon als Trennzeichen\n",
    "        for row in reader:\n",
    "            # Verbinde die Zeilenelemente mit einem Komma, um das Trennzeichen zu vereinheitlichen\n",
    "            unified_row = ','.join(row)\n",
    "            # Ersetze mehrere aufeinander folgende Kommas durch ein einzelnes Komma\n",
    "            unified_row = re.sub(r',+', ',', unified_row)\n",
    "            # Teile die vereinheitlichte Zeile nach dem Komma auf\n",
    "            split_row = unified_row.split(',')\n",
    "            # Entferne die ersten zwei Parameter\n",
    "            cleaned_row = split_row[1:]\n",
    "            # FÃ¼ge die bereinigte Zeile der Liste hinzu\n",
    "            cleaned_data.append(cleaned_row)\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "# Beispiel: Daten aus \"input.csv\" einlesen\n",
    "input_file_path = \"querys_ForcePush.csv\"\n",
    "cleaned_data = read_csv_file(input_file_path)\n",
    "\n",
    "# Bereinigte Daten als pandas DataFrame speichern mit erster Zeile als Spaltennamen\n",
    "df_queried = pd.DataFrame(cleaned_data[1:], columns=cleaned_data[0])\n",
    "\n",
    "\n",
    "# Bereinigte Daten ausgeben\n",
    "# print(df_queried)\n",
    "# df_queried = pd.DataFrame(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Engine speed', 'Engine load', 'Railpressure', 'Air supply',\n",
      "       'Crank angle', 'Intake pressure', 'Back pressure', 'Intake temperature',\n",
      "       'NOx', 'PM 1', 'CO2', 'PM 2', 'Pressure cylinder'],\n",
      "      dtype='object')\n",
      "Index(['Engine speed', 'Engine load', 'Railpressure', 'Air supply',\n",
      "       'Crank angle', 'Intake pressure', 'Back pressure', 'Intake temperature',\n",
      "       'NOx', 'PM 1', 'CO2', 'Pressure cylinder'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def add_data(data, queried_data):\n",
    "    # add queried data (without cost) to initial data\n",
    "    queried_data = queried_data[queried_data['costs'] != 1]\n",
    "    data = pd.concat([data, queried_data.iloc[:, :13]], axis=0)\n",
    "    return data\n",
    "\n",
    "data = add_data(data, df_queried)\n",
    "# print(data)\n",
    "\n",
    "data = data.astype(float)\n",
    "print(data.columns)\n",
    "# drop PM2 column\n",
    "data = data.drop(columns=['PM 2'])\n",
    "print(data.columns)\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to handle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_data(data, row):\n",
    "    x1, x2, x3, x4, x5, x6, x7, x8 = data.iloc[row, 0], data.iloc[row, 1], data.iloc[row, 2], data.iloc[row, 3], data.iloc[row, 4], data.iloc[row, 5], data.iloc[row, 6], data.iloc[row, 7]\n",
    "    return x1, x2, x3, x4, x5, x6, x7, x8\n",
    "\n",
    "def get_critical_output_data(data, i):\n",
    "    x1, x2 = data.iloc[i, 9], data.iloc[i, 11]\n",
    "    return x1, x2\n",
    "\n",
    "# print data column 9 and 11\n",
    "# print(data.iloc[:, 9])\n",
    "# print(data.iloc[:, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:  ['Engine speed' 'Engine load' 'Railpressure' 'Air supply' 'Crank angle'\n",
      " 'Intake pressure' 'Back pressure' 'Intake temperature']\n",
      "Outputs:  ['NOx' 'PM 1' 'CO2' 'Pressure cylinder']\n"
     ]
    }
   ],
   "source": [
    "# put column names into a list\n",
    "column_names = data.columns.values\n",
    "#print(column_names)\n",
    "inputs = column_names[0:8]\n",
    "print(\"Inputs: \", inputs)\n",
    "outputs = column_names[8:12]\n",
    "print(\"Outputs: \", outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify safe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "386    0\n",
      "387    0\n",
      "388    0\n",
      "389    0\n",
      "390    0\n",
      "Name: safe, Length: 391, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# classify data safety\n",
    "# Check if outputs are in safe range\n",
    "# PM 1 < 6, (PM 2 < 16), Pressure cylinder < 160\n",
    "\n",
    "def label_safe(data):\n",
    "    safe = []\n",
    "    for i in range(len(data)):\n",
    "        x9, x10  = get_critical_output_data(data, i)\n",
    "        if x10 < 160:\n",
    "            safe.append(0)\n",
    "        else:\n",
    "            safe.append(2)\n",
    "        \n",
    "\n",
    "    data['safe'] = safe\n",
    "    return data\n",
    "\n",
    "data = label_safe(data)\n",
    "print(data['safe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unsafe in training:  6\n",
      "Number of unsafe in testing:  2\n",
      "Accuracy: 0.9746835443037974\n"
     ]
    }
   ],
   "source": [
    "# train a model to predict unsafe output\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# split data into training and testing data\n",
    "\n",
    "# Schritt 1: Feature-Importance bestimmen\n",
    "X = data[inputs]\n",
    "y = data['safe']\n",
    "#print(y.head())\n",
    "# model = RandomForestRegressor(n_estimators=100)\n",
    "# model.fit(X, y)\n",
    "# feature_importances = model.feature_importances_\n",
    "# print(feature_importances)\n",
    "\n",
    "# # Schritt 2: Datenpunkte basierend auf Feature-Wichtigkeiten anpassen\n",
    "# adjustedData = X * feature_importances\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(adjustedData, y, test_size=0.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#X_train = X_train * feature_importances\n",
    "# count number of False values in y_train and y_test\n",
    "print(\"Number of unsafe in training: \",(len(y_train[y_train == 1])+len(y_train[y_train == 2])))\n",
    "print(\"Number of unsafe in testing: \",(len(y_test[y_test == 1])+len(y_test[y_test == 2])))\n",
    "\n",
    "# train a random forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Submission-Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Engine speed', 'Engine load', 'Railpressure', 'Air supply',\n",
      "       'Crank angle', 'Intake pressure', 'Back pressure',\n",
      "       'Intake temperature'],\n",
      "      dtype='object')\n",
      "   Engine speed  Engine load  Railpressure  Air supply  Crank angle  \\\n",
      "0   2079.242896    15.249091   1059.615682  330.175905    -6.221489   \n",
      "1    820.449848   120.353061   1271.158901  487.645261     3.472795   \n",
      "2   1639.495961    83.300535   1601.817725  634.526366    -0.793814   \n",
      "3   1260.782708   101.754809    828.257891  392.354655     0.122483   \n",
      "4    903.744741    30.628885   1650.039653  602.760187     6.470709   \n",
      "\n",
      "   Intake pressure  Back pressure  Intake temperature  \n",
      "0      1126.065139    3432.348069           73.435318  \n",
      "1      2084.175034    1985.181081           42.863344  \n",
      "2      1268.308573    3234.188679           50.848907  \n",
      "3      2492.140669    1466.726258           72.014909  \n",
      "4      2631.683432    3797.403335           56.749198  \n"
     ]
    }
   ],
   "source": [
    "# import submission data\n",
    "submission_data = pd.read_csv('submission.csv')\n",
    "# print(submission_data)\n",
    "# rename columns\n",
    "submission_data.columns = column_names[:8]\n",
    "print(submission_data.columns)\n",
    "print(submission_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Engine speed  Engine load  Railpressure  Air supply  Crank angle  \\\n",
      "0   2079.242896    15.249091   1059.615682  330.175905    -6.221489   \n",
      "1    820.449848   120.353061   1271.158901  487.645261     3.472795   \n",
      "2   1639.495961    83.300535   1601.817725  634.526366    -0.793814   \n",
      "3   1260.782708   101.754809    828.257891  392.354655     0.122483   \n",
      "4    903.744741    30.628885   1650.039653  602.760187     6.470709   \n",
      "\n",
      "   Intake pressure  Back pressure  Intake temperature  \n",
      "0      1126.065139    3432.348069           73.435318  \n",
      "1      2084.175034    1985.181081           42.863344  \n",
      "2      1268.308573    3234.188679           50.848907  \n",
      "3      2492.140669    1466.726258           72.014909  \n",
      "4      2631.683432    3797.403335           56.749198  \n",
      "[0 2 0 ... 0 0 0]\n",
      "57886\n",
      "57892\n"
     ]
    }
   ],
   "source": [
    "new_data = pd.DataFrame(submission_data, columns=inputs)\n",
    "\n",
    "# check feasibleData safety via classifier\n",
    "X = new_data\n",
    "print(X.head())\n",
    "safeDataPoints = []\n",
    "y_pred = clf.predict(X)\n",
    "print(y_pred)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == 0:\n",
    "        #append the corrsponding data point to safeDataPoints\n",
    "        safeDataPoints.append(X.iloc[i])\n",
    "        \n",
    "print(len(y_pred[y_pred == 0]))\n",
    "print(len(y_pred))\n",
    "# print(len(safeDataPoints) == len(y_pred[y_pred == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find significant data based on complete real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05331485 0.48951586 0.06382142 0.18277145 0.06604942 0.08305314\n",
      " 0.02484484 0.03662903]\n",
      "10473\n",
      "15101\n",
      "7787\n",
      "6372\n",
      "4576\n",
      "13365\n",
      "     Engine speed  Engine load  Railpressure   Air supply  Crank angle  \\\n",
      "172   1024.533330    72.040113   1403.763314   477.502298    -7.769681   \n",
      "174   1284.049344    47.815566   1871.034098   352.713768    -1.206809   \n",
      "176   1191.758251    93.993087   1456.312080   427.813713     2.391920   \n",
      "168   2242.309166    95.669230   2125.724966  1208.109806    -1.861551   \n",
      "216   1739.357208     7.163692   2477.341199   197.665852     0.630771   \n",
      "171   1532.135178    37.604291   2050.359237   805.055858     2.283957   \n",
      "\n",
      "     Intake pressure  Back pressure  Intake temperature  cluster  \n",
      "172      2752.206943    3859.646492           64.728768      0.0  \n",
      "174      2260.923929    3633.288193           63.835354      1.0  \n",
      "176      2835.515555    2016.329443           39.463835      2.0  \n",
      "168      2994.345297    2919.615742           44.292534      3.0  \n",
      "216       982.794388    2445.081046           71.755518      4.0  \n",
      "171      1366.412076    2962.350621           58.315348      5.0  \n",
      "     Engine speed  Engine load  Railpressure   Air supply  Crank angle  \\\n",
      "172   1024.533330    72.040113   1403.763314   477.502298    -7.769681   \n",
      "174   1284.049344    47.815566   1871.034098   352.713768    -1.206809   \n",
      "176   1191.758251    93.993087   1456.312080   427.813713     2.391920   \n",
      "168   2242.309166    95.669230   2125.724966  1208.109806    -1.861551   \n",
      "216   1739.357208     7.163692   2477.341199   197.665852     0.630771   \n",
      "\n",
      "     Intake pressure  Back pressure  Intake temperature  \n",
      "172      2752.206943    3859.646492           64.728768  \n",
      "174      2260.923929    3633.288193           63.835354  \n",
      "176      2835.515555    2016.329443           39.463835  \n",
      "168      2994.345297    2919.615742           44.292534  \n",
      "216       982.794388    2445.081046           71.755518  \n",
      "[0 0 0 0 0 0]\n",
      "True\n",
      "1024.5333299040794;72.0401134786152;1403.763314113594;477.5022984539165;-7.769680616911501;2752.206942924594;3859.646492312073;64.72876798334858\n",
      "1284.0493440628052;47.81556627713144;1871.0340981980085;352.7137680409033;-1.2068088562227786;2260.9239294198605;3633.288193210637;63.83535363491113\n",
      "1191.7582511901855;93.99308656885064;1456.3120798106866;427.8137131914978;2.3919199965894222;2835.5155548792272;2016.3294434582624;39.463834642114726\n",
      "2242.309166491032;95.669230241097;2125.724965978517;1208.109805903974;-1.861551116707851;2994.345297157826;2919.615742479734;44.29253384210615\n",
      "1739.357207715511;7.163692379592007;2477.3411985925286;197.66585226912;0.6307707959243913;982.7943882157326;2445.0810464307306;71.75551761177991\n",
      "1532.1351781487465;37.60429108515382;2050.359236829117;805.0558583366773;2.283956712386539;1366.4120760169062;2962.3506208429862;58.31534771659578\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Schritt 1: Feature-Importance bestimmen\n",
    "complData = data\n",
    "X = complData[inputs]\n",
    "y = complData[outputs]\n",
    "#print(y.head())\n",
    "model = RandomForestRegressor(n_estimators=100)\n",
    "model.fit(X, y)\n",
    "feature_importances = model.feature_importances_\n",
    "print(feature_importances)\n",
    "\n",
    "# Schritt 2: Datenpunkte basierend auf Feature-Wichtigkeiten anpassen\n",
    "adjustedData = X * feature_importances\n",
    "\n",
    "# Schritt 3: Clusteranalyse durchfÃ¼hren mit angepassten Daten\n",
    "scaler = StandardScaler()\n",
    "scaledAdjustedData = scaler.fit_transform(adjustedData)\n",
    "\n",
    "\n",
    "# elbow method to determine number of clusters\n",
    "# from matplotlib import pyplot as plt\n",
    "# inertia = []\n",
    "# for i in range(1, 10):\n",
    "#     kmeans = KMeans(n_clusters=i)\n",
    "#     kmeans.fit(scaledAdjustedData)\n",
    "#     print(i, kmeans.inertia_)\n",
    "#     inertia.append(kmeans.inertia_)\n",
    "# plt.plot(range(1, 10), inertia)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=6)\n",
    "clusterLabels = kmeans.fit_predict(scaledAdjustedData)\n",
    "\n",
    "# Schritt 4: ReprÃ¤sentative Datenpunkte auswÃ¤hlen\n",
    "# (Beispiel zeigt, wie man die Clusterlabels zu den ursprÃ¼nglichen Daten hinzufÃ¼gt und reprÃ¤sentative Punkte auswÃ¤hlt)\n",
    "# freeData = submission_data.droprows(), wenn die Datenpunkte in data enthalten sind\n",
    "\n",
    "# Angenommen, `inputs` ist eine Liste der Spaltennamen, die fÃ¼r den Vergleich verwendet werden sollen\n",
    "# Erstelle eine temporÃ¤re Kopie von `submission_data`, um die Originaldaten nicht zu verÃ¤ndern\n",
    "submission_data_filtered = submission_data.copy()\n",
    "\n",
    "# Schritt 1: Finde Duplikate basierend auf den `inputs` Spalten\n",
    "# Dieser Schritt erzeugt eine Maske (einen Boolean-Array), der fÃ¼r jede Zeile in `submission_data` True ist, wenn diese Zeile in `data` vorhanden ist\n",
    "is_duplicate = submission_data_filtered[inputs].apply(tuple, 1).isin(data[inputs].apply(tuple, 1))\n",
    "\n",
    "# Schritt 2: LÃ¶sche die Reihen aus `submission_data_filtered`, die in `data` vorhanden sind\n",
    "submission_data_filtered = submission_data_filtered[~is_duplicate]\n",
    "\n",
    "#print len of is duplicates True\n",
    "#print(len(is_duplicate[is_duplicate == True]))\n",
    "\n",
    "#print(len(submission_data_filtered), len(submission_data), len(data),len(is_duplicate[is_duplicate == True]), len(submission_data)- len(submission_data_filtered))\n",
    "\n",
    "# `submission_data_filtered` enthÃ¤lt jetzt nur die Reihen, die nicht in `data[inputs]` vorhanden sind\n",
    "\n",
    "submission_data_filtered['cluster'] = kmeans.predict(scaler.transform(submission_data_filtered[inputs] * feature_importances))\n",
    "new_points = []\n",
    "for cluster in range(6):\n",
    "    cluster_data = submission_data_filtered[submission_data_filtered['cluster'] == cluster]\n",
    "    cluster_center = kmeans.cluster_centers_[cluster]\n",
    "    # Berechne die Distanz unter BerÃ¼cksichtigung der Feature-Wichtigkeiten\n",
    "    closest_points, _ = pairwise_distances_argmin_min(cluster_data[inputs] * feature_importances, [cluster_center])\n",
    "    print(len(closest_points))\n",
    "    # get the first 3 closest points\n",
    "    if len(closest_points) > 0:\n",
    "        new_points.append(cluster_data.iloc[closest_points[0]])\n",
    "new_points = pd.DataFrame(new_points)\n",
    "print(new_points)\n",
    "\n",
    "new_data = pd.DataFrame(new_points, columns=inputs)\n",
    "\n",
    "# check feasibleData safety via classifier\n",
    "X = new_data\n",
    "print(X.head())\n",
    "safeDataPoints = []\n",
    "y_pred = clf.predict(X)\n",
    "print(y_pred)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == 0:\n",
    "        #append the corrsponding data point to safeDataPoints\n",
    "        safeDataPoints.append(X.iloc[i])\n",
    "        \n",
    "\n",
    "print(len(safeDataPoints) == len(y_pred[y_pred == 0]))\n",
    "\n",
    "# print as formatted string seperated by semi-colon\n",
    "def print_data(data):\n",
    "    for i in range(len(data)):\n",
    "        print(';'.join(map(str, data[i])))\n",
    "    return\n",
    "\n",
    "print_data(safeDataPoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 5.496636872815159\n",
      "accuracy = 52.98702092799051\n",
      "accuracy = 36.50670924888863\n",
      "accuracy = 81.8811884043256\n",
      "Accuracy:  [5.496636872815159, 52.98702092799051, 36.50670924888863, 81.8811884043256]\n",
      "accuracy = 5.496636872815159\n",
      "accuracy = 52.98702092799051\n",
      "accuracy = 36.50670924888863\n",
      "accuracy = 81.8811884043256\n",
      "Predictions:  [[188.84403717   1.59882677  46.1129744  117.18889439]\n",
      " [184.0122705    1.40370375  44.90566777 110.42829553]\n",
      " [195.17891659   1.37578714  52.61286946 121.20992343]\n",
      " [193.12831671   2.98238515  56.03586893 116.21766234]\n",
      " [167.56302505   0.23928356  38.90021414  86.35516201]\n",
      " [184.95728465   0.87468786  48.79883374  93.52674518]]\n"
     ]
    }
   ],
   "source": [
    "# SVR from Philip\n",
    "from SVRFromPhilip import getSVRPrediction\n",
    "#from SVRCluster import getSVRPrediction\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# # print(getSVRPrediction(safeDataPoints))\n",
    "# getSVRPrediction(safeDataPoints)\n",
    "print(\"Accuracy: \", getSVRPrediction(safeDataPoints)[0])\n",
    "print(\"Predictions: \", getSVRPrediction(safeDataPoints)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  [31.32496643115931, 34.21039679493548, 82.48276936715895, 97.39519012349669]\n",
      "Predictions:  [[  96.03814631   -6.59615324    7.99221953  174.65470073]\n",
      " [   6.28993908   -7.61882718  -11.47694625  174.90129907]\n",
      " [ 441.14876217   -6.85510325   -7.62953728  118.30250369]\n",
      " [ 485.25343886    3.01234017  120.17256826  133.08709984]\n",
      " [-256.43620928   -2.25871652    7.09079735  109.97098748]\n",
      " [ -10.38171798    6.02152518  117.78325609  144.28550474]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\boeke\\Desktop\\Datensicherheit\\force-push\\PLSFromSimon.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data = pd.concat([data, queried_data.iloc[:, :13]], axis=0)\n",
      "c:\\Users\\boeke\\Desktop\\Datensicherheit\\force-push\\PLSFromSimon.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data = pd.concat([data, queried_data.iloc[:, :13]], axis=0)\n"
     ]
    }
   ],
   "source": [
    "# Model from Simon\n",
    "from PLSFromSimon import getPLSPrediction\n",
    "\n",
    "# print(getPLSPrediction(safeDataPoints))\n",
    "print(\"Accuracy: \", getPLSPrediction(safeDataPoints)[0])\n",
    "print(\"Predictions: \", getPLSPrediction(safeDataPoints)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  [32.738754823131316, 29.9325285676926, 81.78687330165893, 97.20252272153884]\n",
      "Predictions:  [[  91.69658743   -3.95365141   33.73469716  159.00357185]\n",
      " [   3.37029781   -4.83940057   14.94498048  156.36597513]\n",
      " [ 457.33020468   -3.13960901   33.91233465  128.10665897]\n",
      " [ 493.10485074    4.22865511  135.429447    139.97526761]\n",
      " [-262.99017274   -1.58509801   10.82437554   91.56133137]\n",
      " [ -13.29290205    4.80198896  102.89686642  130.39626697]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\boeke\\Desktop\\Datensicherheit\\force-push\\tryRidge.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data = pd.concat([data, queried_data.iloc[:, :13]], axis=0)\n",
      "c:\\Users\\boeke\\Desktop\\Datensicherheit\\force-push\\tryRidge.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data = pd.concat([data, queried_data.iloc[:, :13]], axis=0)\n"
     ]
    }
   ],
   "source": [
    "from tryRidge import getRidgePrediction\n",
    "\n",
    "print(\"Accuracy: \", getRidgePrediction(safeDataPoints)[0])\n",
    "print(\"Predictions: \", getRidgePrediction(safeDataPoints)[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
