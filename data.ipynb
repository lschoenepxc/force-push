{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start: Analysiere Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import initial_data.csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from feasibility import is_feasible\n",
    "\n",
    "# import data from csv file\n",
    "def import_data():\n",
    "    data = pd.read_csv('initial_data.csv')\n",
    "    return data\n",
    "\n",
    "data = import_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import queried data\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def read_csv_file(file_path):\n",
    "    # Initialisiere eine leere Liste, um die bereinigten Daten zu speichern\n",
    "    cleaned_data = []\n",
    "\n",
    "    # Lese die CSV-Datei\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')  # Annahme: Semikolon als Trennzeichen\n",
    "        for row in reader:\n",
    "            # Verbinde die Zeilenelemente mit einem Komma, um das Trennzeichen zu vereinheitlichen\n",
    "            unified_row = ','.join(row)\n",
    "            # Ersetze mehrere aufeinander folgende Kommas durch ein einzelnes Komma\n",
    "            unified_row = re.sub(r',+', ',', unified_row)\n",
    "            # Teile die vereinheitlichte Zeile nach dem Komma auf\n",
    "            split_row = unified_row.split(',')\n",
    "            # Entferne die ersten zwei Parameter\n",
    "            cleaned_row = split_row[1:]\n",
    "            # Füge die bereinigte Zeile der Liste hinzu\n",
    "            cleaned_data.append(cleaned_row)\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "# Beispiel: Daten aus \"input.csv\" einlesen\n",
    "input_file_path = \"querys_ForcePush.csv\"\n",
    "cleaned_data = read_csv_file(input_file_path)\n",
    "\n",
    "# Bereinigte Daten als pandas DataFrame speichern mit erster Zeile als Spaltennamen\n",
    "df_queried = pd.DataFrame(cleaned_data[1:], columns=cleaned_data[0])\n",
    "\n",
    "\n",
    "# Bereinigte Daten ausgeben\n",
    "# print(df_queried)\n",
    "# df_queried = pd.DataFrame(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Engine speed', 'Engine load', 'Railpressure', 'Air supply',\n",
      "       'Crank angle', 'Intake pressure', 'Back pressure', 'Intake temperature',\n",
      "       'NOx', 'PM 1', 'CO2', 'PM 2', 'Pressure cylinder'],\n",
      "      dtype='object')\n",
      "Index(['Engine speed', 'Engine load', 'Railpressure', 'Air supply',\n",
      "       'Crank angle', 'Intake pressure', 'Back pressure', 'Intake temperature',\n",
      "       'NOx', 'PM 1', 'CO2', 'Pressure cylinder'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def add_data(data, queried_data):\n",
    "    # add queried data (without cost) to initial data\n",
    "    data = pd.concat([data, queried_data.iloc[:, :13]], axis=0)\n",
    "    return data\n",
    "\n",
    "data = add_data(data, df_queried)\n",
    "# print(data)\n",
    "\n",
    "data = data.astype(float)\n",
    "print(data.columns)\n",
    "# drop PM2 column\n",
    "data = data.drop(columns=['PM 2'])\n",
    "print(data.columns)\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to handle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_data(data, row):\n",
    "    x1, x2, x3, x4, x5, x6, x7, x8 = data.iloc[row, 0], data.iloc[row, 1], data.iloc[row, 2], data.iloc[row, 3], data.iloc[row, 4], data.iloc[row, 5], data.iloc[row, 6], data.iloc[row, 7]\n",
    "    return x1, x2, x3, x4, x5, x6, x7, x8\n",
    "\n",
    "def get_critical_output_data(data, i):\n",
    "    x1, x2 = data.iloc[i, 9], data.iloc[i, 11]\n",
    "    return x1, x2\n",
    "\n",
    "# print data column 9 and 11\n",
    "# print(data.iloc[:, 9])\n",
    "# print(data.iloc[:, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:  ['Engine speed' 'Engine load' 'Railpressure' 'Air supply' 'Crank angle'\n",
      " 'Intake pressure' 'Back pressure' 'Intake temperature']\n",
      "Outputs:  ['NOx' 'PM 1' 'CO2' 'Pressure cylinder']\n"
     ]
    }
   ],
   "source": [
    "# put column names into a list\n",
    "column_names = data.columns.values\n",
    "#print(column_names)\n",
    "inputs = column_names[0:8]\n",
    "print(\"Inputs: \", inputs)\n",
    "outputs = column_names[8:12]\n",
    "print(\"Outputs: \", outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify safe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "158    0\n",
      "159    2\n",
      "160    0\n",
      "161    0\n",
      "162    0\n",
      "Name: safe, Length: 163, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# classify data safety\n",
    "# Check if outputs are in safe range\n",
    "# PM 1 < 6, (PM 2 < 16), Pressure cylinder < 160\n",
    "\n",
    "def label_safe(data):\n",
    "    safe = []\n",
    "    for i in range(len(data)):\n",
    "        x9, x10  = get_critical_output_data(data, i)\n",
    "        if x9 < 6:\n",
    "            if x10 < 160:\n",
    "                safe.append(0)\n",
    "            else:\n",
    "                safe.append(2)\n",
    "        else:\n",
    "            if x10 < 160:\n",
    "                safe.append(1)\n",
    "            else:\n",
    "                safe.append(2)\n",
    "    data['safe'] = safe\n",
    "    return data\n",
    "\n",
    "data = label_safe(data)\n",
    "print(data['safe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unsafe in training:  11\n",
      "Number of unsafe in testing:  3\n",
      "Accuracy: 0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "# train a model to predict unsafe output\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# split data into training and testing data\n",
    "\n",
    "# Schritt 1: Feature-Importance bestimmen\n",
    "X = data[inputs]\n",
    "y = data['safe']\n",
    "#print(y.head())\n",
    "# model = RandomForestRegressor(n_estimators=100)\n",
    "# model.fit(X, y)\n",
    "# feature_importances = model.feature_importances_\n",
    "# print(feature_importances)\n",
    "\n",
    "# # Schritt 2: Datenpunkte basierend auf Feature-Wichtigkeiten anpassen\n",
    "# adjustedData = X * feature_importances\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(adjustedData, y, test_size=0.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#X_train = X_train * feature_importances\n",
    "# count number of False values in y_train and y_test\n",
    "print(\"Number of unsafe in training: \",(len(y_train[y_train == 1])+len(y_train[y_train == 2])))\n",
    "print(\"Number of unsafe in testing: \",(len(y_test[y_test == 1])+len(y_test[y_test == 2])))\n",
    "\n",
    "# train a random forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Submission-Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Engine speed', 'Engine load', 'Railpressure', 'Air supply',\n",
      "       'Crank angle', 'Intake pressure', 'Back pressure',\n",
      "       'Intake temperature'],\n",
      "      dtype='object')\n",
      "   Engine speed  Engine load  Railpressure  Air supply  Crank angle  \\\n",
      "0   2079.242896    15.249091   1059.615682  330.175905    -6.221489   \n",
      "1    820.449848   120.353061   1271.158901  487.645261     3.472795   \n",
      "2   1639.495961    83.300535   1601.817725  634.526366    -0.793814   \n",
      "3   1260.782708   101.754809    828.257891  392.354655     0.122483   \n",
      "4    903.744741    30.628885   1650.039653  602.760187     6.470709   \n",
      "\n",
      "   Intake pressure  Back pressure  Intake temperature  \n",
      "0      1126.065139    3432.348069           73.435318  \n",
      "1      2084.175034    1985.181081           42.863344  \n",
      "2      1268.308573    3234.188679           50.848907  \n",
      "3      2492.140669    1466.726258           72.014909  \n",
      "4      2631.683432    3797.403335           56.749198  \n"
     ]
    }
   ],
   "source": [
    "# import submission data\n",
    "submission_data = pd.read_csv('submission.csv')\n",
    "# print(submission_data)\n",
    "# rename columns\n",
    "submission_data.columns = column_names[:8]\n",
    "print(submission_data.columns)\n",
    "print(submission_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Engine speed  Engine load  Railpressure  Air supply  Crank angle  \\\n",
      "0   2079.242896    15.249091   1059.615682  330.175905    -6.221489   \n",
      "1    820.449848   120.353061   1271.158901  487.645261     3.472795   \n",
      "2   1639.495961    83.300535   1601.817725  634.526366    -0.793814   \n",
      "3   1260.782708   101.754809    828.257891  392.354655     0.122483   \n",
      "4    903.744741    30.628885   1650.039653  602.760187     6.470709   \n",
      "\n",
      "   Intake pressure  Back pressure  Intake temperature  \n",
      "0      1126.065139    3432.348069           73.435318  \n",
      "1      2084.175034    1985.181081           42.863344  \n",
      "2      1268.308573    3234.188679           50.848907  \n",
      "3      2492.140669    1466.726258           72.014909  \n",
      "4      2631.683432    3797.403335           56.749198  \n",
      "[0 0 0 ... 0 0 0]\n",
      "57478\n",
      "57892\n"
     ]
    }
   ],
   "source": [
    "new_data = pd.DataFrame(submission_data, columns=inputs)\n",
    "\n",
    "# check feasibleData safety via classifier\n",
    "X = new_data\n",
    "print(X.head())\n",
    "safeDataPoints = []\n",
    "y_pred = clf.predict(X)\n",
    "print(y_pred)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == 0:\n",
    "        #append the corrsponding data point to safeDataPoints\n",
    "        safeDataPoints.append(X.iloc[i])\n",
    "        \n",
    "print(len(y_pred[y_pred == 0]))\n",
    "print(len(y_pred))\n",
    "# print(len(safeDataPoints) == len(y_pred[y_pred == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find significant data based on complete real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01989823 0.56142685 0.05476143 0.07431341 0.09229494 0.08564932\n",
      " 0.03309019 0.07856563]\n",
      "5663\n",
      "3779\n",
      "9435\n",
      "16794\n",
      "3036\n",
      "19154\n",
      "    Engine speed  Engine load  Railpressure  Air supply  Crank angle  \\\n",
      "12   1911.406720    97.893128    852.417958  958.113591    -0.131095   \n",
      "11    988.948241    38.966357   1471.254208  179.178156     0.869117   \n",
      "23   1414.956297    52.064432    637.906118  911.010384    -3.168024   \n",
      "0    2079.242896    15.249091   1059.615682  330.175905    -6.221489   \n",
      "39   1608.491692    18.610722   1134.982264  230.543761     2.512165   \n",
      "3    1260.782708   101.754809    828.257891  392.354655     0.122483   \n",
      "\n",
      "    Intake pressure  Back pressure  Intake temperature  cluster  \n",
      "12      2238.063748    3652.007343           76.263746      0.0  \n",
      "11      1186.477575    2334.776826           51.094320      1.0  \n",
      "23      1726.091088    1635.553227           46.924010      2.0  \n",
      "0       1126.065139    3432.348069           73.435318      3.0  \n",
      "39      2015.323742    1137.073658           57.149739      4.0  \n",
      "3       2492.140669    1466.726258           72.014909      5.0  \n",
      "    Engine speed  Engine load  Railpressure  Air supply  Crank angle  \\\n",
      "12   1911.406720    97.893128    852.417958  958.113591    -0.131095   \n",
      "11    988.948241    38.966357   1471.254208  179.178156     0.869117   \n",
      "23   1414.956297    52.064432    637.906118  911.010384    -3.168024   \n",
      "0    2079.242896    15.249091   1059.615682  330.175905    -6.221489   \n",
      "39   1608.491692    18.610722   1134.982264  230.543761     2.512165   \n",
      "\n",
      "    Intake pressure  Back pressure  Intake temperature  \n",
      "12      2238.063748    3652.007343           76.263746  \n",
      "11      1186.477575    2334.776826           51.094320  \n",
      "23      1726.091088    1635.553227           46.924010  \n",
      "0       1126.065139    3432.348069           73.435318  \n",
      "39      2015.323742    1137.073658           57.149739  \n",
      "[0 0 0 0 0 0]\n",
      "True\n",
      "1911.4067196846008;97.89312792536816;852.4179576938819;958.113591194255;-0.1310947577770864;2238.0637479638435;3652.0073428215705;76.26374620947573\n",
      "988.9482408761978;38.96635679889533;1471.2542082281043;179.17815555396663;0.8691165200434625;1186.477574725457;2334.7768262827503;51.09432000244607\n",
      "1414.956296980381;52.064431784674525;637.9061175606587;911.0103843653476;-3.1680236602160914;1726.0910879335695;1635.5532267131955;46.92401016399516\n",
      "2079.2428955435757;15.249090547463874;1059.615681686995;330.1759046565936;-6.22148924234347;1126.0651390827563;3432.3480694537734;73.43531762230961\n",
      "1608.491691946983;18.610722429405463;1134.9822642011825;230.54376131809929;2.5121653320374886;2015.3237415572;1137.0736575952446;57.14973865937755\n",
      "1260.7827082276344;101.75480875186622;828.2578910320299;392.35465490079326;0.1224834681488573;2492.140669195358;1466.7262576370772;72.01490949956667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Schritt 1: Feature-Importance bestimmen\n",
    "complData = data\n",
    "X = complData[inputs]\n",
    "y = complData[outputs]\n",
    "#print(y.head())\n",
    "model = RandomForestRegressor(n_estimators=100)\n",
    "model.fit(X, y)\n",
    "feature_importances = model.feature_importances_\n",
    "print(feature_importances)\n",
    "\n",
    "# Schritt 2: Datenpunkte basierend auf Feature-Wichtigkeiten anpassen\n",
    "adjustedData = X * feature_importances\n",
    "\n",
    "# Schritt 3: Clusteranalyse durchführen mit angepassten Daten\n",
    "scaler = StandardScaler()\n",
    "scaledAdjustedData = scaler.fit_transform(adjustedData)\n",
    "\n",
    "\n",
    "# elbow method to determine number of clusters\n",
    "# from matplotlib import pyplot as plt\n",
    "# inertia = []\n",
    "# for i in range(1, 10):\n",
    "#     kmeans = KMeans(n_clusters=i)\n",
    "#     kmeans.fit(scaledAdjustedData)\n",
    "#     print(i, kmeans.inertia_)\n",
    "#     inertia.append(kmeans.inertia_)\n",
    "# plt.plot(range(1, 10), inertia)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=6)\n",
    "clusterLabels = kmeans.fit_predict(scaledAdjustedData)\n",
    "\n",
    "# Schritt 4: Repräsentative Datenpunkte auswählen\n",
    "# (Beispiel zeigt, wie man die Clusterlabels zu den ursprünglichen Daten hinzufügt und repräsentative Punkte auswählt)\n",
    "# freeData = submission_data.droprows(), wenn die Datenpunkte in data enthalten sind\n",
    "\n",
    "# Angenommen, `inputs` ist eine Liste der Spaltennamen, die für den Vergleich verwendet werden sollen\n",
    "# Erstelle eine temporäre Kopie von `submission_data`, um die Originaldaten nicht zu verändern\n",
    "submission_data_filtered = submission_data.copy()\n",
    "\n",
    "# Schritt 1: Finde Duplikate basierend auf den `inputs` Spalten\n",
    "# Dieser Schritt erzeugt eine Maske (einen Boolean-Array), der für jede Zeile in `submission_data` True ist, wenn diese Zeile in `data` vorhanden ist\n",
    "is_duplicate = submission_data_filtered[inputs].apply(tuple, 1).isin(data[inputs].apply(tuple, 1))\n",
    "\n",
    "# Schritt 2: Lösche die Reihen aus `submission_data_filtered`, die in `data` vorhanden sind\n",
    "submission_data_filtered = submission_data_filtered[~is_duplicate]\n",
    "\n",
    "#print len of is duplicates True\n",
    "#print(len(is_duplicate[is_duplicate == True]))\n",
    "\n",
    "#print(len(submission_data_filtered), len(submission_data), len(data),len(is_duplicate[is_duplicate == True]), len(submission_data)- len(submission_data_filtered))\n",
    "\n",
    "# `submission_data_filtered` enthält jetzt nur die Reihen, die nicht in `data[inputs]` vorhanden sind\n",
    "\n",
    "submission_data_filtered['cluster'] = kmeans.predict(scaler.transform(submission_data_filtered[inputs] * feature_importances))\n",
    "new_points = []\n",
    "for cluster in range(6):\n",
    "    cluster_data = submission_data_filtered[submission_data_filtered['cluster'] == cluster]\n",
    "    cluster_center = kmeans.cluster_centers_[cluster]\n",
    "    # Berechne die Distanz unter Berücksichtigung der Feature-Wichtigkeiten\n",
    "    closest_points, _ = pairwise_distances_argmin_min(cluster_data[inputs] * feature_importances, [cluster_center])\n",
    "    print(len(closest_points))\n",
    "    # get the first 3 closest points\n",
    "    if len(closest_points) > 0:\n",
    "        new_points.append(cluster_data.iloc[closest_points[0]])\n",
    "new_points = pd.DataFrame(new_points)\n",
    "print(new_points)\n",
    "\n",
    "new_data = pd.DataFrame(new_points, columns=inputs)\n",
    "\n",
    "# check feasibleData safety via classifier\n",
    "X = new_data\n",
    "print(X.head())\n",
    "safeDataPoints = []\n",
    "y_pred = clf.predict(X)\n",
    "print(y_pred)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == 0:\n",
    "        #append the corrsponding data point to safeDataPoints\n",
    "        safeDataPoints.append(X.iloc[i])\n",
    "        \n",
    "\n",
    "print(len(safeDataPoints) == len(y_pred[y_pred == 0]))\n",
    "\n",
    "# print as formatted string seperated by semi-colon\n",
    "def print_data(data):\n",
    "    for i in range(len(data)):\n",
    "        print(';'.join(map(str, data[i])))\n",
    "    return\n",
    "\n",
    "print_data(safeDataPoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  [84.65667347794793, 36.49911892967837, 89.42462870367467, 96.96934705222473]\n",
      "Predictions:  [[2.24641234e+02 1.80341203e-01 7.87108190e+01 1.06964842e+02]\n",
      " [1.51722806e+02 3.13486978e+00 4.09172491e+01 8.03351874e+01]\n",
      " [2.37802669e+02 2.23774121e+00 6.23715916e+01 9.31151664e+01]\n",
      " [1.82866074e+02 1.05547346e+00 6.04729580e+01 9.00591211e+01]\n",
      " [9.66161735e+01 3.39503337e+00 3.09684347e+01 6.11943280e+01]\n",
      " [2.04207379e+02 4.47766785e+00 7.20452881e+01 8.98503262e+01]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\boeke\\Desktop\\Datensicherheit\\force-push\\SVRFromPhilip.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data = pd.concat([data, queried_data.iloc[:, :13]], axis=0)\n",
      "c:\\Users\\boeke\\Desktop\\Datensicherheit\\force-push\\SVRFromPhilip.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data = pd.concat([data, queried_data.iloc[:, :13]], axis=0)\n"
     ]
    }
   ],
   "source": [
    "# SVR from Philip\n",
    "from SVRFromPhilip import getSVRPrediction\n",
    "\n",
    "# print(getSVRPrediction(safeDataPoints))\n",
    "print(\"Accuracy: \", getSVRPrediction(safeDataPoints)[0])\n",
    "print(\"Predictions: \", getSVRPrediction(safeDataPoints)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  [31.32496643115931, 34.21039679493548, 82.48276936715895, 97.39519012349669]\n",
      "Predictions:  [[ 4.19432940e+02  5.76979012e+00  1.42162160e+02  1.59652917e+02]\n",
      " [ 2.82171700e+01 -2.97225243e+00  3.28750651e+00  1.29007215e+02]\n",
      " [ 3.35685066e+02  8.49724998e+00  1.18841583e+02  6.61667174e+01]\n",
      " [ 6.44267794e+01 -3.54980771e+00 -7.67290085e-01  1.37415811e+02]\n",
      " [ 1.70134523e+02 -5.82182533e+00 -4.59678677e+01  4.49158868e+01]\n",
      " [ 3.48697175e+02  1.90426326e-01  4.90260174e+01  6.26566158e+01]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\boeke\\Desktop\\Datensicherheit\\force-push\\PLSFromSimon.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data = pd.concat([data, queried_data.iloc[:, :13]], axis=0)\n",
      "c:\\Users\\boeke\\Desktop\\Datensicherheit\\force-push\\PLSFromSimon.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data = pd.concat([data, queried_data.iloc[:, :13]], axis=0)\n"
     ]
    }
   ],
   "source": [
    "# Model from Simon\n",
    "from PLSFromSimon import getPLSPrediction\n",
    "\n",
    "# print(getPLSPrediction(safeDataPoints))\n",
    "print(\"Accuracy: \", getPLSPrediction(safeDataPoints)[0])\n",
    "print(\"Predictions: \", getPLSPrediction(safeDataPoints)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'getPolyRegPrediction' from 'poly_reg' (c:\\Users\\boeke\\Desktop\\Datensicherheit\\force-push\\poly_reg.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpoly_reg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getPolyRegPrediction\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;124m\"\u001b[39m, getPolyRegPrediction(safeDataPoints)[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions: \u001b[39m\u001b[38;5;124m\"\u001b[39m, getPolyRegPrediction(safeDataPoints)[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'getPolyRegPrediction' from 'poly_reg' (c:\\Users\\boeke\\Desktop\\Datensicherheit\\force-push\\poly_reg.py)"
     ]
    }
   ],
   "source": [
    "from polyReg import getPolyRegPrediction\n",
    "\n",
    "print(\"Accuracy: \", getPolyRegPrediction(safeDataPoints)[0])\n",
    "print(\"Predictions: \", getPolyRegPrediction(safeDataPoints)[1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
