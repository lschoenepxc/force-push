{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start: Analysiere Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import initial_data.csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from feasibility import is_feasible\n",
    "\n",
    "# import data from csv file\n",
    "def import_data():\n",
    "    data = pd.read_csv('initial_data.csv')\n",
    "    return data\n",
    "\n",
    "data = import_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import queried data\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def read_csv_file(file_path):\n",
    "    # Initialisiere eine leere Liste, um die bereinigten Daten zu speichern\n",
    "    cleaned_data = []\n",
    "\n",
    "    # Lese die CSV-Datei\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')  # Annahme: Semikolon als Trennzeichen\n",
    "        for row in reader:\n",
    "            # Verbinde die Zeilenelemente mit einem Komma, um das Trennzeichen zu vereinheitlichen\n",
    "            unified_row = ','.join(row)\n",
    "            # Ersetze mehrere aufeinander folgende Kommas durch ein einzelnes Komma\n",
    "            unified_row = re.sub(r',+', ',', unified_row)\n",
    "            # Teile die vereinheitlichte Zeile nach dem Komma auf\n",
    "            split_row = unified_row.split(',')\n",
    "            # Entferne die ersten zwei Parameter\n",
    "            cleaned_row = split_row[1:]\n",
    "            # Füge die bereinigte Zeile der Liste hinzu\n",
    "            cleaned_data.append(cleaned_row)\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "# Beispiel: Daten aus \"input.csv\" einlesen\n",
    "input_file_path = \"querys_ForcePush.csv\"\n",
    "cleaned_data = read_csv_file(input_file_path)\n",
    "\n",
    "# Bereinigte Daten als pandas DataFrame speichern mit erster Zeile als Spaltennamen\n",
    "df_queried = pd.DataFrame(cleaned_data[1:], columns=cleaned_data[0])\n",
    "\n",
    "\n",
    "# Bereinigte Daten ausgeben\n",
    "# print(df_queried)\n",
    "# df_queried = pd.DataFrame(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Engine speed', 'Engine load', 'Railpressure', 'Air supply',\n",
      "       'Crank angle', 'Intake pressure', 'Back pressure', 'Intake temperature',\n",
      "       'NOx', 'PM 1', 'CO2', 'PM 2', 'Pressure cylinder'],\n",
      "      dtype='object')\n",
      "Index(['Engine speed', 'Engine load', 'Railpressure', 'Air supply',\n",
      "       'Crank angle', 'Intake pressure', 'Back pressure', 'Intake temperature',\n",
      "       'NOx', 'PM 1', 'CO2', 'Pressure cylinder'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def add_data(data, queried_data):\n",
    "    # add queried data (without cost) to initial data\n",
    "    data = pd.concat([data, queried_data.iloc[:, :13]], axis=0)\n",
    "    return data\n",
    "\n",
    "data = add_data(data, df_queried)\n",
    "# print(data)\n",
    "\n",
    "data = data.astype(float)\n",
    "print(data.columns)\n",
    "# drop PM2 column\n",
    "data = data.drop(columns=['PM 2'])\n",
    "print(data.columns)\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to handle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_data(data, row):\n",
    "    x1, x2, x3, x4, x5, x6, x7, x8 = data.iloc[row, 0], data.iloc[row, 1], data.iloc[row, 2], data.iloc[row, 3], data.iloc[row, 4], data.iloc[row, 5], data.iloc[row, 6], data.iloc[row, 7]\n",
    "    return x1, x2, x3, x4, x5, x6, x7, x8\n",
    "\n",
    "def get_critical_output_data(data, i):\n",
    "    x1, x2 = data.iloc[i, 9], data.iloc[i, 11]\n",
    "    return x1, x2\n",
    "\n",
    "# print data column 9 and 11\n",
    "# print(data.iloc[:, 9])\n",
    "# print(data.iloc[:, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:  ['Engine speed' 'Engine load' 'Railpressure' 'Air supply' 'Crank angle'\n",
      " 'Intake pressure' 'Back pressure' 'Intake temperature']\n",
      "Outputs:  ['NOx' 'PM 1' 'CO2' 'Pressure cylinder']\n"
     ]
    }
   ],
   "source": [
    "# put column names into a list\n",
    "column_names = data.columns.values\n",
    "#print(column_names)\n",
    "inputs = column_names[0:8]\n",
    "print(\"Inputs: \", inputs)\n",
    "outputs = column_names[8:12]\n",
    "print(\"Outputs: \", outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify safe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "158    0\n",
      "159    2\n",
      "160    0\n",
      "161    0\n",
      "162    0\n",
      "Name: safe, Length: 163, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# classify data safety\n",
    "# Check if outputs are in safe range\n",
    "# PM 1 < 6, (PM 2 < 16), Pressure cylinder < 160\n",
    "\n",
    "def label_safe(data):\n",
    "    safe = []\n",
    "    for i in range(len(data)):\n",
    "        x9, x10  = get_critical_output_data(data, i)\n",
    "        if x9 < 6:\n",
    "            if x10 < 160:\n",
    "                safe.append(0)\n",
    "            else:\n",
    "                safe.append(2)\n",
    "        else:\n",
    "            if x10 < 160:\n",
    "                safe.append(1)\n",
    "            else:\n",
    "                safe.append(2)\n",
    "    data['safe'] = safe\n",
    "    return data\n",
    "\n",
    "data = label_safe(data)\n",
    "print(data['safe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unsafe in training:  11\n",
      "Number of unsafe in testing:  3\n",
      "Accuracy: 0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "# train a model to predict unsafe output\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# split data into training and testing data\n",
    "\n",
    "# Schritt 1: Feature-Importance bestimmen\n",
    "X = data[inputs]\n",
    "y = data['safe']\n",
    "#print(y.head())\n",
    "# model = RandomForestRegressor(n_estimators=100)\n",
    "# model.fit(X, y)\n",
    "# feature_importances = model.feature_importances_\n",
    "# print(feature_importances)\n",
    "\n",
    "# # Schritt 2: Datenpunkte basierend auf Feature-Wichtigkeiten anpassen\n",
    "# adjustedData = X * feature_importances\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(adjustedData, y, test_size=0.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#X_train = X_train * feature_importances\n",
    "# count number of False values in y_train and y_test\n",
    "print(\"Number of unsafe in training: \",(len(y_train[y_train == 1])+len(y_train[y_train == 2])))\n",
    "print(\"Number of unsafe in testing: \",(len(y_test[y_test == 1])+len(y_test[y_test == 2])))\n",
    "\n",
    "# train a random forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Submission-Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Engine speed', 'Engine load', 'Railpressure', 'Air supply',\n",
      "       'Crank angle', 'Intake pressure', 'Back pressure',\n",
      "       'Intake temperature'],\n",
      "      dtype='object')\n",
      "   Engine speed  Engine load  Railpressure  Air supply  Crank angle  \\\n",
      "0   2079.242896    15.249091   1059.615682  330.175905    -6.221489   \n",
      "1    820.449848   120.353061   1271.158901  487.645261     3.472795   \n",
      "2   1639.495961    83.300535   1601.817725  634.526366    -0.793814   \n",
      "3   1260.782708   101.754809    828.257891  392.354655     0.122483   \n",
      "4    903.744741    30.628885   1650.039653  602.760187     6.470709   \n",
      "\n",
      "   Intake pressure  Back pressure  Intake temperature  \n",
      "0      1126.065139    3432.348069           73.435318  \n",
      "1      2084.175034    1985.181081           42.863344  \n",
      "2      1268.308573    3234.188679           50.848907  \n",
      "3      2492.140669    1466.726258           72.014909  \n",
      "4      2631.683432    3797.403335           56.749198  \n"
     ]
    }
   ],
   "source": [
    "# import submission data\n",
    "submission_data = pd.read_csv('submission.csv')\n",
    "# print(submission_data)\n",
    "# rename columns\n",
    "submission_data.columns = column_names[:8]\n",
    "print(submission_data.columns)\n",
    "print(submission_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Engine speed  Engine load  Railpressure  Air supply  Crank angle  \\\n",
      "0   2079.242896    15.249091   1059.615682  330.175905    -6.221489   \n",
      "1    820.449848   120.353061   1271.158901  487.645261     3.472795   \n",
      "2   1639.495961    83.300535   1601.817725  634.526366    -0.793814   \n",
      "3   1260.782708   101.754809    828.257891  392.354655     0.122483   \n",
      "4    903.744741    30.628885   1650.039653  602.760187     6.470709   \n",
      "\n",
      "   Intake pressure  Back pressure  Intake temperature  \n",
      "0      1126.065139    3432.348069           73.435318  \n",
      "1      2084.175034    1985.181081           42.863344  \n",
      "2      1268.308573    3234.188679           50.848907  \n",
      "3      2492.140669    1466.726258           72.014909  \n",
      "4      2631.683432    3797.403335           56.749198  \n",
      "[0 0 0 ... 0 0 0]\n",
      "57603\n",
      "57892\n"
     ]
    }
   ],
   "source": [
    "new_data = pd.DataFrame(submission_data, columns=inputs)\n",
    "\n",
    "# check feasibleData safety via classifier\n",
    "X = new_data\n",
    "print(X.head())\n",
    "safeDataPoints = []\n",
    "y_pred = clf.predict(X)\n",
    "print(y_pred)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == 0:\n",
    "        #append the corrsponding data point to safeDataPoints\n",
    "        safeDataPoints.append(X.iloc[i])\n",
    "        \n",
    "print(len(y_pred[y_pred == 0]))\n",
    "print(len(y_pred))\n",
    "# print(len(safeDataPoints) == len(y_pred[y_pred == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find significant data based on complete real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02260869 0.56672235 0.04831596 0.10177777 0.09264129 0.07691565\n",
      " 0.0326077  0.0584106 ]\n",
      "11459\n",
      "12325\n",
      "4200\n",
      "16870\n",
      "9110\n",
      "3878\n",
      "    Engine speed  Engine load  Railpressure  Air supply  Crank angle  \\\n",
      "2    1639.495961    83.300535   1601.817725  634.526366    -0.793814   \n",
      "23   1414.956297    52.064432    637.906118  911.010384    -3.168024   \n",
      "11    988.948241    38.966357   1471.254208  179.178156     0.869117   \n",
      "3    1260.782708   101.754809    828.257891  392.354655     0.122483   \n",
      "10   1769.902128   125.412489   1832.109282  560.685582    -7.100246   \n",
      "20   1237.667766    20.297782   2036.500320  565.600433    -0.005135   \n",
      "\n",
      "    Intake pressure  Back pressure  Intake temperature  cluster  \n",
      "2       1268.308573    3234.188679           50.848907      0.0  \n",
      "23      1726.091088    1635.553227           46.924010      1.0  \n",
      "11      1186.477575    2334.776826           51.094320      2.0  \n",
      "3       2492.140669    1466.726258           72.014909      3.0  \n",
      "10      2624.872050    2734.767688           47.136546      4.0  \n",
      "20      1331.763297    1937.479623           70.902561      5.0  \n",
      "    Engine speed  Engine load  Railpressure  Air supply  Crank angle  \\\n",
      "2    1639.495961    83.300535   1601.817725  634.526366    -0.793814   \n",
      "23   1414.956297    52.064432    637.906118  911.010384    -3.168024   \n",
      "11    988.948241    38.966357   1471.254208  179.178156     0.869117   \n",
      "3    1260.782708   101.754809    828.257891  392.354655     0.122483   \n",
      "10   1769.902128   125.412489   1832.109282  560.685582    -7.100246   \n",
      "\n",
      "    Intake pressure  Back pressure  Intake temperature  \n",
      "2       1268.308573    3234.188679           50.848907  \n",
      "23      1726.091088    1635.553227           46.924010  \n",
      "11      1186.477575    2334.776826           51.094320  \n",
      "3       2492.140669    1466.726258           72.014909  \n",
      "10      2624.872050    2734.767688           47.136546  \n",
      "[0 0 0 0 0 0]\n",
      "True\n",
      "1639.495961368084;83.30053522160668;1601.8177249196335;634.5263661353208;-0.7938141296199053;1268.3085728382453;3234.188678692356;50.84890673909587\n",
      "1414.956296980381;52.064431784674525;637.9061175606587;911.0103843653476;-3.1680236602160914;1726.0910879335695;1635.5532267131955;46.92401016399516\n",
      "988.9482408761978;38.96635679889533;1471.2542082281043;179.17815555396663;0.8691165200434625;1186.477574725457;2334.7768262827503;51.09432000244607\n",
      "1260.7827082276344;101.75480875186622;828.2578910320299;392.35465490079326;0.1224834681488573;2492.140669195358;1466.7262576370772;72.01490949956667\n",
      "1769.902127981186;125.41248921303216;1832.1092817357544;560.6855824714153;-7.100246476648422;2624.8720504292887;2734.7676883194176;47.13654642356207\n",
      "1237.667766213417;20.29778233729303;2036.5003196091689;565.6004329788941;-0.0051345769315958;1331.763297068353;1937.479623173178;70.90256134332367\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Schritt 1: Feature-Importance bestimmen\n",
    "complData = data\n",
    "X = complData[inputs]\n",
    "y = complData[outputs]\n",
    "#print(y.head())\n",
    "model = RandomForestRegressor(n_estimators=100)\n",
    "model.fit(X, y)\n",
    "feature_importances = model.feature_importances_\n",
    "print(feature_importances)\n",
    "\n",
    "# Schritt 2: Datenpunkte basierend auf Feature-Wichtigkeiten anpassen\n",
    "adjustedData = X * feature_importances\n",
    "\n",
    "# Schritt 3: Clusteranalyse durchführen mit angepassten Daten\n",
    "scaler = StandardScaler()\n",
    "scaledAdjustedData = scaler.fit_transform(adjustedData)\n",
    "\n",
    "\n",
    "# elbow method to determine number of clusters\n",
    "# from matplotlib import pyplot as plt\n",
    "# inertia = []\n",
    "# for i in range(1, 10):\n",
    "#     kmeans = KMeans(n_clusters=i)\n",
    "#     kmeans.fit(scaledAdjustedData)\n",
    "#     print(i, kmeans.inertia_)\n",
    "#     inertia.append(kmeans.inertia_)\n",
    "# plt.plot(range(1, 10), inertia)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=6)\n",
    "clusterLabels = kmeans.fit_predict(scaledAdjustedData)\n",
    "\n",
    "# Schritt 4: Repräsentative Datenpunkte auswählen\n",
    "# (Beispiel zeigt, wie man die Clusterlabels zu den ursprünglichen Daten hinzufügt und repräsentative Punkte auswählt)\n",
    "# freeData = submission_data.droprows(), wenn die Datenpunkte in data enthalten sind\n",
    "\n",
    "# Angenommen, `inputs` ist eine Liste der Spaltennamen, die für den Vergleich verwendet werden sollen\n",
    "# Erstelle eine temporäre Kopie von `submission_data`, um die Originaldaten nicht zu verändern\n",
    "submission_data_filtered = submission_data.copy()\n",
    "\n",
    "# Schritt 1: Finde Duplikate basierend auf den `inputs` Spalten\n",
    "# Dieser Schritt erzeugt eine Maske (einen Boolean-Array), der für jede Zeile in `submission_data` True ist, wenn diese Zeile in `data` vorhanden ist\n",
    "is_duplicate = submission_data_filtered[inputs].apply(tuple, 1).isin(data[inputs].apply(tuple, 1))\n",
    "\n",
    "# Schritt 2: Lösche die Reihen aus `submission_data_filtered`, die in `data` vorhanden sind\n",
    "submission_data_filtered = submission_data_filtered[~is_duplicate]\n",
    "\n",
    "#print len of is duplicates True\n",
    "#print(len(is_duplicate[is_duplicate == True]))\n",
    "\n",
    "#print(len(submission_data_filtered), len(submission_data), len(data),len(is_duplicate[is_duplicate == True]), len(submission_data)- len(submission_data_filtered))\n",
    "\n",
    "# `submission_data_filtered` enthält jetzt nur die Reihen, die nicht in `data[inputs]` vorhanden sind\n",
    "\n",
    "submission_data_filtered['cluster'] = kmeans.predict(scaler.transform(submission_data_filtered[inputs] * feature_importances))\n",
    "new_points = []\n",
    "for cluster in range(6):\n",
    "    cluster_data = submission_data_filtered[submission_data_filtered['cluster'] == cluster]\n",
    "    cluster_center = kmeans.cluster_centers_[cluster]\n",
    "    # Berechne die Distanz unter Berücksichtigung der Feature-Wichtigkeiten\n",
    "    closest_points, _ = pairwise_distances_argmin_min(cluster_data[inputs] * feature_importances, [cluster_center])\n",
    "    print(len(closest_points))\n",
    "    # get the first 3 closest points\n",
    "    if len(closest_points) > 0:\n",
    "        new_points.append(cluster_data.iloc[closest_points[0]])\n",
    "new_points = pd.DataFrame(new_points)\n",
    "print(new_points)\n",
    "\n",
    "new_data = pd.DataFrame(new_points, columns=inputs)\n",
    "\n",
    "# check feasibleData safety via classifier\n",
    "X = new_data\n",
    "print(X.head())\n",
    "safeDataPoints = []\n",
    "y_pred = clf.predict(X)\n",
    "print(y_pred)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == 0:\n",
    "        #append the corrsponding data point to safeDataPoints\n",
    "        safeDataPoints.append(X.iloc[i])\n",
    "        \n",
    "\n",
    "print(len(safeDataPoints) == len(y_pred[y_pred == 0]))\n",
    "\n",
    "# print as formatted string seperated by semi-colon\n",
    "def print_data(data):\n",
    "    for i in range(len(data)):\n",
    "        print(';'.join(map(str, data[i])))\n",
    "    return\n",
    "\n",
    "print_data(safeDataPoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[195.71545533   1.64247422  73.86443233  90.94153567]\n",
      " [212.44813097   4.05380409  57.14384688  81.22528774]\n",
      " [122.89219122  -2.13045578  38.41671539  84.26910539]\n",
      " [176.13133841   4.6967428   66.88798898  87.28860583]\n",
      " [203.99749423  -0.43923745  79.39915147  99.09117486]\n",
      " [ 95.69610809   0.9088034   44.52071777  71.55267431]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauraschone/Desktop/repos/force-push/SVRFromPhilip.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data = pd.concat([data, queried_data.iloc[:, :13]], axis=0)\n"
     ]
    }
   ],
   "source": [
    "# SVR from Philip\n",
    "from SVRFromPhilip import getSVRPrediction\n",
    "\n",
    "# print(getSVRPrediction(safeDataPoints))\n",
    "print(\"Accuracy: \", getSVRPrediction(safeDataPoints)[0])\n",
    "print(\"Predictions: \", getSVRPrediction(safeDataPoints)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 110.13204234    5.38986015   97.2377105   177.4448024 ]\n",
      " [ 368.73169243    9.10900691  123.52346415   63.14871484]\n",
      " [ -44.41789004   -2.46512238    1.25148401  132.36842738]\n",
      " [ 438.50066435   -0.63633089   51.48858132   58.77772373]\n",
      " [ 472.2250241    -3.71175414   42.21875752  135.24567892]\n",
      " [-229.26128268    6.1642975    91.09156481   79.94081225]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauraschone/Desktop/repos/force-push/PLSFromSimon.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data = pd.concat([data, queried_data.iloc[:, :13]], axis=0)\n",
      "/Users/lauraschone/Desktop/repos/force-push/.venv/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but PLSRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Model from Simon\n",
    "from PLSFromSimon import getPLSPrediction\n",
    "\n",
    "# print(getPLSPrediction(safeDataPoints))\n",
    "print(\"Accuracy: \", getPLSPrediction(safeDataPoints)[0])\n",
    "print(\"Predictions: \", getPLSPrediction(safeDataPoints)[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
