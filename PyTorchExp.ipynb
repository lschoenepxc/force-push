{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Beispielhafte Daten\n",
    "# X = np.random.rand(100, 13)\n",
    "# y = np.random.rand(100, 5)\n",
    "\n",
    "# Deine echten Daten hier laden\n",
    "X = ... # Dein Input-Datensatz (100, 13)\n",
    "y = ... # Dein Output-Datensatz (100, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOx                  685.846823\n",
      "PM 1                   5.547569\n",
      "CO2                  160.188892\n",
      "PM 2                  15.561156\n",
      "Pressure cylinder    115.101073\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"initial_data.csv\")\n",
    "\n",
    "#print(data)\n",
    "#print(data.keys())\n",
    "\n",
    "x = data[['Engine speed', 'Engine load', 'Railpressure', 'Air supply', 'Crank angle', 'Intake pressure', 'Back pressure', 'Intake temperature']]\n",
    "y = data[['NOx', 'PM 1', 'CO2', 'PM 2', 'Pressure cylinder']]\n",
    "\n",
    "x_max = x.max()\n",
    "x_min = x.min()\n",
    "y_max = y.max()\n",
    "y_min = y.min()\n",
    "\n",
    "#print(str(y_min) + \"\\n:\\n\" + str(y_max))\n",
    "\n",
    "y_range = y_max - y_min\n",
    "\n",
    "print(y_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datenaufteilung in Training und Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "# Datenstandardisierung\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "y_train = scaler_y.fit_transform(y_train)\n",
    "y_test = scaler_y.transform(y_test)\n",
    "\n",
    "\n",
    "\n",
    "# Umwandlung in PyTorch Tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(8, 16)\n",
    "        #self.layer2 = nn.Linear(16, 8)\n",
    "        #self.layer3 = nn.Linear(12, 10)\n",
    "        self.output = nn.Linear(16, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        #x = self.relu(self.layer2(x))\n",
    "        #x = self.relu(self.layer3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verlustfunktion und Optimierer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Trainingsparameter\n",
    "num_epochs = 300\n",
    "batch_size = 3\n",
    "\n",
    "# Daten in Batches aufteilen\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300], Loss: 0.1544\n",
      "Epoch [20/300], Loss: 0.4962\n",
      "Epoch [30/300], Loss: 0.2150\n",
      "Epoch [40/300], Loss: 0.2123\n",
      "Epoch [50/300], Loss: 0.0738\n",
      "Epoch [60/300], Loss: 0.0439\n",
      "Epoch [70/300], Loss: 0.1704\n",
      "Epoch [80/300], Loss: 0.1086\n",
      "Epoch [90/300], Loss: 0.0547\n",
      "Epoch [100/300], Loss: 0.0738\n",
      "Epoch [110/300], Loss: 0.0492\n",
      "Epoch [120/300], Loss: 0.0633\n",
      "Epoch [130/300], Loss: 0.7510\n",
      "Epoch [140/300], Loss: 0.1396\n",
      "Epoch [150/300], Loss: 0.1153\n",
      "Epoch [160/300], Loss: 0.1087\n",
      "Epoch [170/300], Loss: 0.0085\n",
      "Epoch [180/300], Loss: 0.1225\n",
      "Epoch [190/300], Loss: 0.1965\n",
      "Epoch [200/300], Loss: 0.0312\n",
      "Epoch [210/300], Loss: 0.0650\n",
      "Epoch [220/300], Loss: 0.0729\n",
      "Epoch [230/300], Loss: 0.1363\n",
      "Epoch [240/300], Loss: 0.0602\n",
      "Epoch [250/300], Loss: 0.0186\n",
      "Epoch [260/300], Loss: 0.0745\n",
      "Epoch [270/300], Loss: 0.0865\n",
      "Epoch [280/300], Loss: 0.1886\n",
      "Epoch [290/300], Loss: 0.0634\n",
      "Epoch [300/300], Loss: 0.0570\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        # Forward-Pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward-Pass und Optimierung\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Modell speichern (optional)\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4354\n",
      "[38.88923029  0.52117083 18.8422769   2.35521152 54.30891802]:[45.11063706  0.37043239 13.33908759  1.34438471 50.75279046]\n",
      "[127.99717254   2.99342779  65.13084665   9.69170347  72.02820745]:[144.31417996   3.2442979   63.07336518   9.38207516  71.47676054]\n",
      "[78.55949241  1.94996514 33.92522264  6.62800625 62.22061233]:[66.86942417  4.09588526 36.29146793 16.28325513 60.49792953]\n",
      "[80.50035026  2.22188088 43.89848757  8.81374164 62.35524058]:[94.23299035  2.83292106 38.50904745 10.53630092 59.95670593]\n",
      "[50.40468278  1.2468531  23.54815474  5.81114129 54.86618818]:[41.90699086  0.76024322 22.40993205  4.24715129 54.66871374]\n",
      "[94.7411168   2.69419485 49.13469223 10.61561499 64.76352359]:[84.29989805  3.41757813 48.34347086 14.36933964 66.03415315]\n",
      "[530.35526748   1.08458558  75.13174254   2.1682828  108.21763291]:[465.21158656   0.99647182  76.01429871   2.44630606 107.39016145]\n",
      "[195.12089255   3.21622458 100.53495453   5.49137409 102.65177744]:[165.12383548   3.63098576 104.66582744   7.39466323  99.42011942]\n",
      "[13.71310493 -0.05731885  3.35088012  3.03684816 52.14118411]:[22.09589639  0.19848545  3.68189957  1.95892246 45.7710704 ]\n",
      "[ 5.7337956  -0.71254978 -1.35342063  1.31826023 49.91479374]:[17.97783801  0.15113867  2.89162952  1.69033535 44.94630108]\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Setzt das Modell in den Evaluationsmodus\n",
    "with torch.no_grad():  # Keine Gradientenberechnung\n",
    "    y_pred = model(X_test)\n",
    "    test_loss = criterion(y_pred, y_test)\n",
    "    #print(y_test)\n",
    "    \n",
    "    print(f'Test Loss: {test_loss.item():.4f}')\n",
    "    y_pred = scaler_y.inverse_transform(y_pred)\n",
    "    y_test = scaler_y.inverse_transform(y_test)\n",
    "    for pred, test in zip(y_pred, y_test):\n",
    "        print(str(pred) + \":\" + str(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build matrix of elementwise mean squared error\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "matrix = []\n",
    "for pred, test in zip(y_pred, y_test):\n",
    "    data_set_mse = []\n",
    "    for p, t in zip(pred, test):\n",
    "        #print(str(pred) + \":\" + str(test))\n",
    "        mse = mean_squared_error([p], [t])\n",
    "        data_set_mse.append(mse)\n",
    "    #print(data_set_mse)\n",
    "\n",
    "    percentage_to_range = data_set_mse / y_range\n",
    "    matrix.append(percentage_to_range)\n",
    "#print(matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0.08502407021302955\n",
      "0.08695451669311216\n",
      "1.41617646269859\n",
      "0.1528513855982927\n",
      "0.06271746070604456\n",
      "0.23534108975632845\n",
      "1.2409420039928099\n",
      "0.3546098705108222\n",
      "0.10843040615314829\n",
      "0.13778299559181134\n",
      "Mean:\n",
      "0.3880830261913989\n"
     ]
    }
   ],
   "source": [
    "# check mse per dataset row\n",
    "\n",
    "from statistics import mean \n",
    "\n",
    "mean_for_predicted_dataset = []\n",
    "\n",
    "for predicted_dataset in matrix:\n",
    "    mean_for_predicted_dataset.append(mean(predicted_dataset))\n",
    "\n",
    "print(\"\\n\")\n",
    "for entry in mean_for_predicted_dataset:\n",
    "    print(entry)\n",
    "\n",
    "print(\"Mean:\")\n",
    "print(mean(mean_for_predicted_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[617.5133914290598, 0.6815258139403898, 10.705812730275088, 11.884536564535889, 9.971682765266163]\n",
      "NOx                  685.846823\n",
      "PM 1                   5.547569\n",
      "CO2                  160.188892\n",
      "PM 2                  15.561156\n",
      "Pressure cylinder    115.101073\n",
      "dtype: float64\n",
      "mse per feature in percentage to value range\n",
      "NOx                  0.900366\n",
      "PM 1                 0.122851\n",
      "CO2                  0.066832\n",
      "PM 2                 0.763731\n",
      "Pressure cylinder    0.086634\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Ceck Accuracy among features of test data\n",
    "\n",
    "#print(y_pred.shape)\n",
    "\n",
    "y_pred_t = np.transpose(y_pred)\n",
    "y_test_t = np.transpose(y_test)\n",
    "\n",
    "# print(y_pred_t.shape)\n",
    "# print(y_test_t.shape)\n",
    "featurewise_mean = []\n",
    "for feature_t, feature_p in zip(y_test_t, y_pred_t):\n",
    "    # print(feature_t)\n",
    "    # print(feature_p)\n",
    "    # print(mean_squared_error(feature_t, feature_p))\n",
    "    featurewise_mean.append(mean_squared_error(feature_t, feature_p))\n",
    "\n",
    "print(featurewise_mean)\n",
    "\n",
    "print(y_range)\n",
    "featurewise_mean_percentage = featurewise_mean / y_range\n",
    "print(\"mse per feature in percentage to value range\")\n",
    "print(featurewise_mean_percentage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
