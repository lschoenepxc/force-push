{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Beispielhafte Daten\n",
    "# X = np.random.rand(100, 13)\n",
    "# y = np.random.rand(100, 5)\n",
    "\n",
    "# Deine echten Daten hier laden\n",
    "X = ... # Dein Input-Datensatz (100, 13)\n",
    "y = ... # Dein Output-Datensatz (100, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "def read_csv_file(file_path):\n",
    "    # Initialisiere eine leere Liste, um die bereinigten Daten zu speichern\n",
    "    cleaned_data = []\n",
    "\n",
    "    # Lese die CSV-Datei\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')  # Annahme: Semikolon als Trennzeichen\n",
    "        for row in reader:\n",
    "            # Verbinde die Zeilenelemente mit einem Komma, um das Trennzeichen zu vereinheitlichen\n",
    "            unified_row = ','.join(row)\n",
    "            # Ersetze mehrere aufeinander folgende Kommas durch ein einzelnes Komma\n",
    "            unified_row = re.sub(r',+', ',', unified_row)\n",
    "            # Teile die vereinheitlichte Zeile nach dem Komma auf\n",
    "            split_row = unified_row.split(',')\n",
    "            # Entferne die ersten zwei Parameter\n",
    "            cleaned_row = split_row[1:]\n",
    "            # FÃ¼ge die bereinigte Zeile der Liste hinzu\n",
    "            cleaned_data.append(cleaned_row)\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "# Beispiel: Daten aus \"input.csv\" einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     index  Engine speed  Engine load  Railpressure  Air supply  Crank angle  \\\n",
      "0      0.0         700.0         7.33    500.899994   78.375298         1.91   \n",
      "1      1.0         700.0        25.85    577.599976  118.067299         0.94   \n",
      "2      2.0         700.0    46.669998    615.799988  144.355698         2.86   \n",
      "3      3.0         700.0    69.550003    778.299988  174.177902         2.22   \n",
      "4      4.0         700.0    94.330002    900.099976  213.314804        -1.72   \n",
      "..     ...           ...          ...           ...         ...          ...   \n",
      "108    8.0   2183.257812    47.490673   1444.360107  650.760376     4.574534   \n",
      "109    9.0   1896.591187    97.303398   1351.508545  827.040039    -3.222039   \n",
      "110   10.0   1199.474487   101.517929   1868.645874  393.788116    -0.035194   \n",
      "111   11.0     903.48175    10.912282    960.143188  602.400879     4.090658   \n",
      "112   12.0    989.338928    81.168571    798.997742  610.851624     4.238798   \n",
      "\n",
      "     Intake pressure  Back pressure  Intake temperature         NOx      PM 1  \\\n",
      "0              967.5    1027.300049                65.5   17.977839  0.151139   \n",
      "1        1017.599976    1063.800049           53.400002   30.613787  0.421432   \n",
      "2        1086.699951    1156.199951           47.900002  109.199806  0.767395   \n",
      "3        1171.099976    1215.099976           42.700001  186.406723  0.658505   \n",
      "4        1313.300049    1305.900024           38.299999  315.430176  0.773289   \n",
      "..               ...            ...                 ...         ...       ...   \n",
      "108      1214.924438    2534.904785           63.454926  267.957947  1.561381   \n",
      "109      1957.858154    2360.668213           61.206924  655.742188  3.415683   \n",
      "110      2519.380859    1297.593384           53.867203  318.380676  8.514029   \n",
      "111      2214.485596    1118.469849           65.710732   97.881203  0.366846   \n",
      "112      2893.275635    3159.828369           46.599705  790.772034  0.453512   \n",
      "\n",
      "            CO2       PM 2  Pressure cylinder  \n",
      "0      2.891628   1.690336          44.946301  \n",
      "1     10.976312   4.617809          55.885777  \n",
      "2     18.844496   5.139177          73.029762  \n",
      "3     27.739765   4.104144          86.322105  \n",
      "4     39.269863   4.298426           93.84214  \n",
      "..          ...        ...                ...  \n",
      "108   48.390953   0.092351          68.638382  \n",
      "109  108.400719  42.976513         104.524902  \n",
      "110   83.169342   0.008595         114.401657  \n",
      "111    9.534796   1.038233         108.758553  \n",
      "112   42.087067   0.912289         161.766251  \n",
      "\n",
      "[113 rows x 14 columns]\n",
      "NOx                   774.920654\n",
      "PM 1                   13.661119\n",
      "CO2                   160.188889\n",
      "PM 2                 5505.038574\n",
      "Pressure cylinder     117.810837\n",
      "dtype: Float32\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"initial_data.csv\")\n",
    "cleaned_data = read_csv_file(\"querys_ForcePush.csv\")\n",
    "df_queried = pd.DataFrame(cleaned_data[1:], columns=cleaned_data[0])\n",
    "def add_data(data, queried_data):\n",
    "    # add queried data (without cost) to initial data\n",
    "    data = pd.concat([data, queried_data.iloc[:, :13]], axis=0)\n",
    "\n",
    "    # data = data.append(queried_data, ignore_index=True)\n",
    "    return data\n",
    "\n",
    "data = add_data(data, df_queried)\n",
    "data = data.reset_index().astype('Float32')\n",
    "\n",
    "print(data)\n",
    "\n",
    "#print(data)\n",
    "#print(data.keys())\n",
    "\n",
    "x = data[['Engine speed', 'Engine load', 'Railpressure', 'Air supply', 'Crank angle', 'Intake pressure', 'Back pressure', 'Intake temperature']]\n",
    "y = data[['NOx', 'PM 1', 'CO2', 'PM 2', 'Pressure cylinder']]\n",
    "\n",
    "x_max = x.max()\n",
    "x_min = x.min()\n",
    "y_max = y.max()\n",
    "y_min = y.min()\n",
    "\n",
    "#print(str(y_min) + \"\\n:\\n\" + str(y_max))\n",
    "\n",
    "y_range = y_max - y_min\n",
    "\n",
    "print(y_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datenaufteilung in Training und Test\n",
    "from numpy import double\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "# Datenstandardisierung\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "# X_train = scaler_X.fit_transform(X_train)\n",
    "# X_test = scaler_X.transform(X_test)\n",
    "\n",
    "# y_train = scaler_y.fit_transform(y_train)\n",
    "# y_test = scaler_y.transform(y_test)\n",
    "\n",
    "x_train = np.array(X_train.values, dtype=float)\n",
    "x_test = np.array(X_train.values, dtype=float)\n",
    "y_test = np.array(y_test.values, dtype=float)\n",
    "y_train = np.array(y_train.values, dtype=float)\n",
    "\n",
    "# Umwandlung in PyTorch Tensors\n",
    "X_train = torch.from_numpy(x_train).to(torch.float32)\n",
    "X_test = torch.from_numpy(x_test).to(torch.float32)\n",
    "y_train = torch.from_numpy(y_train).to(torch.float32)\n",
    "y_test = torch.from_numpy(y_test).to(torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(8, 16)\n",
    "        self.layer2 = nn.Linear(16, 12)\n",
    "        self.layer3 = nn.Linear(12, 10)\n",
    "        self.output = nn.Linear(10, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #print(x[0])\n",
    "        mat = torch.diag(torch.tensor([1, 1, 1, 1, 1, 1, 1, 1])).to(torch.float32)\n",
    "        #print(mat[0])\n",
    "        #x = torch.matmul(x, mat)\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verlustfunktion und Optimierer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Trainingsparameter\n",
    "num_epochs = 300\n",
    "batch_size = 3\n",
    "\n",
    "# Daten in Batches aufteilen\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300], Loss: 2196.4773\n",
      "Epoch [20/300], Loss: 2608.5247\n",
      "Epoch [30/300], Loss: 5015.5801\n",
      "Epoch [40/300], Loss: 215.9424\n",
      "Epoch [50/300], Loss: 282.3383\n",
      "Epoch [60/300], Loss: 1278.7155\n",
      "Epoch [70/300], Loss: 155.9314\n",
      "Epoch [80/300], Loss: 6436.4492\n",
      "Epoch [90/300], Loss: 1645.0992\n",
      "Epoch [100/300], Loss: 257.6303\n",
      "Epoch [110/300], Loss: 421.2752\n",
      "Epoch [120/300], Loss: 310894.6562\n",
      "Epoch [130/300], Loss: 551.7656\n",
      "Epoch [140/300], Loss: 382.4821\n",
      "Epoch [150/300], Loss: 2345.7417\n",
      "Epoch [160/300], Loss: 482.4137\n",
      "Epoch [170/300], Loss: 5044.0391\n",
      "Epoch [180/300], Loss: 77.8629\n",
      "Epoch [190/300], Loss: 366.7238\n",
      "Epoch [200/300], Loss: 3683.3008\n",
      "Epoch [210/300], Loss: 67.6539\n",
      "Epoch [220/300], Loss: 805.7157\n",
      "Epoch [230/300], Loss: 350.0794\n",
      "Epoch [240/300], Loss: 3326.8042\n",
      "Epoch [250/300], Loss: 1079.8528\n",
      "Epoch [260/300], Loss: 518.3149\n",
      "Epoch [270/300], Loss: 597.5664\n",
      "Epoch [280/300], Loss: 238.3482\n",
      "Epoch [290/300], Loss: 315.8860\n",
      "Epoch [300/300], Loss: 183.3493\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        # Forward-Pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward-Pass und Optimierung\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Modell speichern (optional)\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([69.1511, -1.4395, 19.4707, 15.2578, 48.6120]):tensor([165.1238,   3.6310, 104.6658,   7.3947,  99.4201])\n",
      "tensor([207.5252,   6.8012,  38.9971, 346.1646,  72.4383]):tensor([315.4302,   0.7733,  39.2699,   4.2984,  93.8421])\n",
      "tensor([77.2682, -0.7009, 24.0888,  6.9952, 47.6075]):tensor([134.1714,   3.2451,  65.6674,  12.9136,  74.5443])\n",
      "tensor([120.4147,   1.8916,  26.5759,  -7.5148,  45.1551]):tensor([50.6014,  0.5372, 16.3025,  2.3297, 53.8660])\n",
      "tensor([171.7396,   0.2309,  58.9660,   5.2868,  91.1544]):tensor([22.0959,  0.1985,  3.6819,  1.9589, 45.7711])\n",
      "tensor([109.0713,  -0.5010,  35.5457,   5.8757,  62.9831]):tensor([94.2330,  2.8329, 38.5090, 10.5363, 59.9567])\n",
      "tensor([304.9307,   3.5228, 113.0875,  15.6768, 140.2809]):tensor([66.8694,  4.0959, 36.2915, 16.2833, 60.4979])\n",
      "tensor([146.2139,   0.3528,  50.4721,   5.8361,  76.9466]):tensor([187.1987,   2.8930,  92.9965,   4.8530, 101.0711])\n",
      "tensor([127.6744,  -3.0959,  35.2981,  13.7912,  90.2829]):tensor([125.6448,   2.2057,  76.3790,   4.4945,  86.9087])\n",
      "tensor([83.3243, -2.0198, 22.8948,  7.1711, 59.4411]):tensor([35.5889,  0.4711, 13.4768,  4.4749, 57.4358])\n",
      "tensor([ 8.8767e+01, -7.0116e-02,  2.9621e+01,  6.7648e+00,  4.9519e+01]):tensor([126.5793,   0.5256,  65.4693,   1.1705,  71.7500])\n",
      "tensor([70.9899, -0.9164, 21.3883,  6.8612, 45.7057]):tensor([381.6186,   1.9227,  98.6851,   4.4769, 116.5188])\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Setzt das Modell in den Evaluationsmodus\n",
    "with torch.no_grad():  # Keine Gradientenberechnung\n",
    "    y_pred = model(X_test)\n",
    "    #test_loss = criterion(y_pred, y_test)\n",
    "    #print(y_test)\n",
    "    \n",
    "    #print(f'Test Loss: {test_loss.item():.4f}')\n",
    "    # y_pred = scaler_y.inverse_transform(y_pred)\n",
    "    # y_test = scaler_y.inverse_transform(y_test)\n",
    "    for pred, test in zip(y_pred, y_test):\n",
    "        print(str(pred) + \":\" + str(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build matrix of elementwise mean squared error\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "matrix = []\n",
    "for pred, test in zip(y_pred, y_test):\n",
    "    data_set_mse = []\n",
    "    for p, t in zip(pred, test):\n",
    "        #print(str(pred) + \":\" + str(test))\n",
    "        mse = mean_squared_error([p], [t])\n",
    "        data_set_mse.append(mse)\n",
    "    #print(data_set_mse)\n",
    "\n",
    "    percentage_to_range = data_set_mse / y_max\n",
    "    matrix.append(percentage_to_range)\n",
    "#print(matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "14.797601823407158\n",
      "8.284228243960555\n",
      "4.063233019071178\n",
      "1.4860651364149182\n",
      "11.958741590728104\n",
      "0.23974098275673733\n",
      "29.440977922822682\n",
      "3.4556864421644264\n",
      "2.4955112239676924\n",
      "0.7803004071220909\n",
      "2.5549172038941474\n",
      "38.048249549081056\n",
      "Mean:\n",
      "9.800437795449229\n"
     ]
    }
   ],
   "source": [
    "# check mse per dataset row\n",
    "\n",
    "from statistics import mean \n",
    "\n",
    "mean_for_predicted_dataset = []\n",
    "\n",
    "for predicted_dataset in matrix:\n",
    "    mean_for_predicted_dataset.append(mean(predicted_dataset))\n",
    "\n",
    "print(\"\\n\")\n",
    "for entry in mean_for_predicted_dataset:\n",
    "    print(entry)\n",
    "\n",
    "print(\"Mean:\")\n",
    "print(mean(mean_for_predicted_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ceck Accuracy among features of test data\n",
    "\n",
    "#print(y_pred.shape)\n",
    "\n",
    "# y_pred_t = np.transpose(y_pred)\n",
    "# y_test_t = np.transpose(y_test)\n",
    "\n",
    "# # print(y_pred_t.shape)\n",
    "# # print(y_test_t.shape)\n",
    "# featurewise_mean = []\n",
    "# for feature_t, feature_p in zip(y_test_t, y_pred_t):\n",
    "#     # print(feature_t)\n",
    "#     # print(feature_p)\n",
    "#     # print(mean_squared_error(feature_t, feature_p))\n",
    "#     featurewise_mean.append(mean_squared_error(feature_t, feature_p))\n",
    "\n",
    "# print(featurewise_mean)\n",
    "\n",
    "# #print(y_range)\n",
    "# featurewise_mean_percentage = featurewise_mean / y_max**2\n",
    "# print(\"mse per feature in percentage to value range\")\n",
    "# print(featurewise_mean_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([182.8054,  -1.4774,  58.2065,  78.3922, 112.2739],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "x = torch.tensor([16.4712565906533, 37.609734587256995, 900.1427871055062, 228.42045796100746, -8.592515890505748, 1981.8799610478413, 3509.5803914574226, 48.709871706251796])\n",
    "testResults = model(x)\n",
    "print(type(testResults))\n",
    "#testResults = scaler_y.inverse_transform(testResults.unsqueeze(0).detach().numpy())\n",
    "print(testResults)\n",
    "#[125.10746278858385, 0.4371585551795759, 20.70223941458411, 3.4443235692265173, 102.99879114216813]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
