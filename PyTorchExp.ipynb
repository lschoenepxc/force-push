{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Beispielhafte Daten\n",
    "# X = np.random.rand(100, 13)\n",
    "# y = np.random.rand(100, 5)\n",
    "\n",
    "# Deine echten Daten hier laden\n",
    "X = ... # Dein Input-Datensatz (100, 13)\n",
    "y = ... # Dein Output-Datensatz (100, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOx                  685.846823\n",
      "PM 1                   5.547569\n",
      "CO2                  160.188892\n",
      "PM 2                  15.561156\n",
      "Pressure cylinder    115.101073\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"initial_data.csv\")\n",
    "\n",
    "#print(data)\n",
    "#print(data.keys())\n",
    "\n",
    "x = data[['Engine speed', 'Engine load', 'Railpressure', 'Air supply', 'Crank angle', 'Intake pressure', 'Back pressure', 'Intake temperature']]\n",
    "y = data[['NOx', 'PM 1', 'CO2', 'PM 2', 'Pressure cylinder']]\n",
    "\n",
    "x_max = x.max()\n",
    "x_min = x.min()\n",
    "y_max = y.max()\n",
    "y_min = y.min()\n",
    "\n",
    "#print(str(y_min) + \"\\n:\\n\" + str(y_max))\n",
    "\n",
    "y_range = y_max - y_min\n",
    "\n",
    "print(y_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datenaufteilung in Training und Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "# Datenstandardisierung\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "y_train = scaler_y.fit_transform(y_train)\n",
    "y_test = scaler_y.transform(y_test)\n",
    "\n",
    "\n",
    "\n",
    "# Umwandlung in PyTorch Tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(8, 16)\n",
    "        self.layer2 = nn.Linear(16, 12)\n",
    "        self.layer3 = nn.Linear(12, 10)\n",
    "        self.output = nn.Linear(10, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verlustfunktion und Optimierer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Trainingsparameter\n",
    "num_epochs = 300\n",
    "batch_size = 3\n",
    "\n",
    "# Daten in Batches aufteilen\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300], Loss: 0.1751\n",
      "Epoch [20/300], Loss: 0.1217\n",
      "Epoch [30/300], Loss: 0.0807\n",
      "Epoch [40/300], Loss: 0.1748\n",
      "Epoch [50/300], Loss: 0.0815\n",
      "Epoch [60/300], Loss: 0.1586\n",
      "Epoch [70/300], Loss: 0.2009\n",
      "Epoch [80/300], Loss: 0.1073\n",
      "Epoch [90/300], Loss: 0.0765\n",
      "Epoch [100/300], Loss: 0.0515\n",
      "Epoch [110/300], Loss: 0.7049\n",
      "Epoch [120/300], Loss: 0.2243\n",
      "Epoch [130/300], Loss: 0.0824\n",
      "Epoch [140/300], Loss: 0.0865\n",
      "Epoch [150/300], Loss: 0.0916\n",
      "Epoch [160/300], Loss: 0.0222\n",
      "Epoch [170/300], Loss: 0.0860\n",
      "Epoch [180/300], Loss: 0.1912\n",
      "Epoch [190/300], Loss: 0.0510\n",
      "Epoch [200/300], Loss: 0.0590\n",
      "Epoch [210/300], Loss: 0.1619\n",
      "Epoch [220/300], Loss: 0.0566\n",
      "Epoch [230/300], Loss: 0.0774\n",
      "Epoch [240/300], Loss: 0.0929\n",
      "Epoch [250/300], Loss: 0.0625\n",
      "Epoch [260/300], Loss: 0.3708\n",
      "Epoch [270/300], Loss: 0.0293\n",
      "Epoch [280/300], Loss: 0.0957\n",
      "Epoch [290/300], Loss: 0.0489\n",
      "Epoch [300/300], Loss: 0.0300\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        # Forward-Pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward-Pass und Optimierung\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Modell speichern (optional)\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3438\n",
      "[41.14299944  0.58214004 21.7872745   2.15640972 55.62369427]:[45.11063706  0.37043239 13.33908759  1.34438471 50.75279046]\n",
      "[131.71369706   3.13554281  62.38156442  10.72217058  70.83469976]:[144.31417996   3.2442979   63.07336518   9.38207516  71.47676054]\n",
      "[65.97137429  1.93152984 36.64003307  7.52564334 59.87109941]:[66.86942417  4.09588526 36.29146793 16.28325513 60.49792953]\n",
      "[67.11721887  2.51848961 38.3940865  10.73083023 60.01219377]:[94.23299035  2.83292106 38.50904745 10.53630092 59.95670593]\n",
      "[49.19917043  0.93842533 20.32735261  4.85778913 56.91254837]:[41.90699086  0.76024322 22.40993205  4.24715129 54.66871374]\n",
      "[89.43849394  2.96160777 48.77547089 12.15301307 63.13492179]:[84.29989805  3.41757813 48.34347086 14.36933964 66.03415315]\n",
      "[563.61712844   1.03111941  78.90135886   2.3484306  110.40902448]:[465.21158656   0.99647182  76.01429871   2.44630606 107.39016145]\n",
      "[195.97349465   3.43658363 101.34686009   6.28731369 102.67766736]:[165.12383548   3.63098576 104.66582744   7.39466323  99.42011942]\n",
      "[14.75568324  0.2529952   5.48944856  3.9174857  49.1153868 ]:[22.09589639  0.19848545  3.68189957  1.95892246 45.7710704 ]\n",
      "[ 6.84036115e-01 -1.16690130e-02 -3.80333887e-01  3.26797512e+00\n",
      "  4.46829102e+01]:[17.97783801  0.15113867  2.89162952  1.69033535 44.94630108]\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Setzt das Modell in den Evaluationsmodus\n",
    "with torch.no_grad():  # Keine Gradientenberechnung\n",
    "    y_pred = model(X_test)\n",
    "    test_loss = criterion(y_pred, y_test)\n",
    "    #print(y_test)\n",
    "    \n",
    "    print(f'Test Loss: {test_loss.item():.4f}')\n",
    "    y_pred = scaler_y.inverse_transform(y_pred)\n",
    "    y_test = scaler_y.inverse_transform(y_test)\n",
    "    for pred, test in zip(y_pred, y_test):\n",
    "        print(str(pred) + \":\" + str(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build matrix of elementwise mean squared error\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "matrix = []\n",
    "for pred, test in zip(y_pred, y_test):\n",
    "    data_set_mse = []\n",
    "    for p, t in zip(pred, test):\n",
    "        #print(str(pred) + \":\" + str(test))\n",
    "        mse = mean_squared_error([p], [t])\n",
    "        data_set_mse.append(mse)\n",
    "    #print(data_set_mse)\n",
    "\n",
    "    percentage_to_range = data_set_mse / y_max\n",
    "    matrix.append(percentage_to_range)\n",
    "#print(matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0.13132318641588256\n",
      "0.0686918512722262\n",
      "1.1072926366140783\n",
      "0.21288881449044245\n",
      "0.03241534084134923\n",
      "0.08586592323399748\n",
      "2.7734928963021312\n",
      "0.31359451587335774\n",
      "0.08051339105120783\n",
      "0.12970280456950534\n",
      "Mean:\n",
      "0.4935781360664178\n"
     ]
    }
   ],
   "source": [
    "# check mse per dataset row\n",
    "\n",
    "from statistics import mean \n",
    "\n",
    "mean_for_predicted_dataset = []\n",
    "\n",
    "for predicted_dataset in matrix:\n",
    "    mean_for_predicted_dataset.append(mean(predicted_dataset))\n",
    "\n",
    "print(\"\\n\")\n",
    "for entry in mean_for_predicted_dataset:\n",
    "    print(entry)\n",
    "\n",
    "print(\"Mean:\")\n",
    "print(mean(mean_for_predicted_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1197.847338205802, 0.5148077444893129, 10.983256378589118, 9.203454804332717, 6.8953256634269335]\n",
      "NOx                  685.846823\n",
      "PM 1                   5.547569\n",
      "CO2                  160.188892\n",
      "PM 2                  15.561156\n",
      "Pressure cylinder    115.101073\n",
      "dtype: float64\n",
      "mse per feature in percentage to value range\n",
      "NOx                  1.701912\n",
      "PM 1                 0.090338\n",
      "CO2                  0.067349\n",
      "PM 2                 0.565210\n",
      "Pressure cylinder    0.043083\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Ceck Accuracy among features of test data\n",
    "\n",
    "#print(y_pred.shape)\n",
    "\n",
    "y_pred_t = np.transpose(y_pred)\n",
    "y_test_t = np.transpose(y_test)\n",
    "\n",
    "# print(y_pred_t.shape)\n",
    "# print(y_test_t.shape)\n",
    "featurewise_mean = []\n",
    "for feature_t, feature_p in zip(y_test_t, y_pred_t):\n",
    "    # print(feature_t)\n",
    "    # print(feature_p)\n",
    "    # print(mean_squared_error(feature_t, feature_p))\n",
    "    featurewise_mean.append(mean_squared_error(feature_t, feature_p))\n",
    "\n",
    "print(featurewise_mean)\n",
    "\n",
    "print(y_range)\n",
    "featurewise_mean_percentage = featurewise_mean / y_max\n",
    "print(\"mse per feature in percentage to value range\")\n",
    "print(featurewise_mean_percentage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
